{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import resource\n",
        "import time"
      ],
      "metadata": {
        "id": "5-zPINznIxZa"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KNN"
      ],
      "metadata": {
        "id": "vGV5pRJk_xp5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Numpy"
      ],
      "metadata": {
        "id": "dBurCNgT_2sf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import resource\n",
        "from collections import Counter\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# --- Load MNIST ---\n",
        "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
        "X, y = mnist[\"data\"], mnist[\"target\"].astype(np.int64)\n",
        "\n",
        "# Normalize to [0, 1]\n",
        "X = X / 255.0\n",
        "\n",
        "# Split into train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=1000, test_size=100, stratify=y, random_state=42)\n",
        "\n",
        "# --- KNN Logic ---\n",
        "def euclidean_distance(a, b):\n",
        "    return np.sqrt(np.sum((a - b)**2, axis=1))\n",
        "\n",
        "def knn_numpy(X_train, y_train, X_test, k=5):\n",
        "    predictions = []\n",
        "    for test_point in X_test:\n",
        "        distances = euclidean_distance(X_train, test_point)\n",
        "        nearest = np.argsort(distances)[:k]\n",
        "        nearest_labels = y_train[nearest]\n",
        "        predictions.append(Counter(nearest_labels).most_common(1)[0][0])\n",
        "    return np.array(predictions)\n",
        "\n",
        "# --- Peak RAM ---\n",
        "def print_peak_ram_usage():\n",
        "    usage = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n",
        "    print(f\"Peak RAM Usage: {usage / 1024:.2f} MB\")\n",
        "\n",
        "# --- Memory Bandwidth ---\n",
        "def test_bandwidth_numpy(X_train, X_test):\n",
        "    start = time.time()\n",
        "    _ = np.sum((X_train - X_test[0]) ** 2, axis=1)\n",
        "    end = time.time()\n",
        "    bytes_moved = (X_train.nbytes + X_test[0].nbytes) * 2\n",
        "    bandwidth = bytes_moved / (end - start) / 1e9\n",
        "    print(f\"Estimated Memory Bandwidth: {bandwidth:.2f} GB/s\")\n",
        "\n",
        "# --- Run Benchmark ---\n",
        "start = time.time()\n",
        "y_pred = knn_numpy(X_train, y_train, X_test)\n",
        "end = time.time()\n",
        "accuracy = np.mean(y_pred == y_test)\n",
        "\n",
        "print(f\"KNN-Numpy Accuracy (MNIST): {accuracy:.2f}\")\n",
        "print(f\"Execution Time: {end - start:.4f} seconds\")\n",
        "print_peak_ram_usage()\n",
        "test_bandwidth_numpy(X_train, X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSqabQ6w_40r",
        "outputId": "799b1ac4-f773-42a2-e1fa-e3a4f66549a1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN-Numpy Accuracy (MNIST): 0.96\n",
            "Execution Time: 0.2646 seconds\n",
            "Peak RAM Usage: 1508.25 MB\n",
            "Estimated Memory Bandwidth: 5.22 GB/s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. PyTorch"
      ],
      "metadata": {
        "id": "xhv9LV-NAT_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "import resource\n",
        "from collections import Counter\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# --- Load MNIST from OpenML ---\n",
        "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
        "X, y = mnist[\"data\"], mnist[\"target\"].astype(int)\n",
        "\n",
        "# Normalize and split\n",
        "X = X / 255.0\n",
        "X_train_np, X_test_np, y_train_np, y_test_np = train_test_split(X, y, train_size=1000, test_size=100, stratify=y, random_state=42)\n",
        "\n",
        "# --- Convert to PyTorch tensors (CPU-only) ---\n",
        "device = \"cpu\"\n",
        "X_train = torch.tensor(X_train_np, dtype=torch.float32, device=device)\n",
        "y_train = torch.tensor(y_train_np, dtype=torch.int64, device=device)\n",
        "X_test = torch.tensor(X_test_np, dtype=torch.float32, device=device)\n",
        "y_test = torch.tensor(y_test_np, dtype=torch.int64, device=device)\n",
        "\n",
        "# --- Euclidean distance ---\n",
        "def euclidean_distance(a, b):\n",
        "    return torch.sqrt(torch.sum((a - b) ** 2, dim=1))\n",
        "\n",
        "# --- KNN Logic ---\n",
        "def knn_torch(X_train, y_train, X_test, k=5):\n",
        "    predictions = []\n",
        "    for i in range(X_test.shape[0]):\n",
        "        test_point = X_test[i]\n",
        "        distances = euclidean_distance(X_train, test_point)\n",
        "        nearest = torch.topk(distances, k=k, largest=False).indices\n",
        "        nearest_labels = y_train[nearest]\n",
        "        predicted = torch.mode(nearest_labels).values.item()\n",
        "        predictions.append(predicted)\n",
        "    return torch.tensor(predictions)\n",
        "\n",
        "# --- Peak RAM ---\n",
        "def print_peak_ram_usage():\n",
        "    usage = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n",
        "    print(f\"[PyTorch] Peak RAM Usage: {usage / 1024:.2f} MB\")\n",
        "\n",
        "# --- Memory Bandwidth ---\n",
        "def test_bandwidth_torch(X_train, X_test):\n",
        "    start = time.time()\n",
        "    _ = torch.sum((X_train - X_test[0]) ** 2, dim=1)\n",
        "    end = time.time()\n",
        "\n",
        "    bytes_moved = (X_train.element_size() * X_train.nelement() +\n",
        "                   X_test[0].element_size() * X_test[0].nelement()) * 2\n",
        "    bandwidth = bytes_moved / (end - start) / 1e9\n",
        "    print(f\"[PyTorch] Estimated Memory Bandwidth: {bandwidth:.2f} GB/s\")\n",
        "\n",
        "# --- Benchmark ---\n",
        "start = time.time()\n",
        "y_pred = knn_torch(X_train, y_train, X_test, k=5)\n",
        "end = time.time()\n",
        "\n",
        "accuracy = (y_pred == y_test).float().mean().item()\n",
        "print(f\"KNN-PyTorch Accuracy (MNIST): {accuracy:.2f}\")\n",
        "print(f\"Execution Time: {end - start:.4f} seconds\")\n",
        "print_peak_ram_usage()\n",
        "test_bandwidth_torch(X_train, X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-wW48BKASVe",
        "outputId": "4158108c-8cd7-4117-c45f-3a2f0fa59db0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN-PyTorch Accuracy (MNIST): 0.96\n",
            "Execution Time: 0.1857 seconds\n",
            "[PyTorch] Peak RAM Usage: 2732.95 MB\n",
            "[PyTorch] Estimated Memory Bandwidth: 3.75 GB/s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. C/C++"
      ],
      "metadata": {
        "id": "DKtvhcHXAncy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Fetch and prepare MNIST\n",
        "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
        "X, y = mnist[\"data\"], mnist[\"target\"].astype(int)\n",
        "X = X / 255.0\n",
        "\n",
        "# Split and save to CSV\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=1000, test_size=100, stratify=y, random_state=42)\n",
        "\n",
        "np.savetxt(\"X_train.csv\", X_train, delimiter=\",\")\n",
        "np.savetxt(\"y_train.csv\", y_train, fmt='%d')\n",
        "np.savetxt(\"X_test.csv\", X_test, delimiter=\",\")\n",
        "np.savetxt(\"y_test.csv\", y_test, fmt='%d')"
      ],
      "metadata": {
        "id": "tRSd08KHuRN0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile knn_cpu.cpp\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <cmath>\n",
        "#include <algorithm>\n",
        "#include <map>\n",
        "#include <chrono>\n",
        "#include <fstream>\n",
        "#include <sstream>\n",
        "#include <sys/resource.h>\n",
        "\n",
        "using namespace std;\n",
        "using namespace std::chrono;\n",
        "\n",
        "// Load CSV into 2D vector\n",
        "vector<vector<double>> load_csv_data(const string& filename, int rows, int cols) {\n",
        "    vector<vector<double>> data(rows, vector<double>(cols));\n",
        "    ifstream file(filename);\n",
        "    string line;\n",
        "    for (int i = 0; i < rows && getline(file, line); ++i) {\n",
        "        stringstream ss(line);\n",
        "        string val;\n",
        "        for (int j = 0; j < cols && getline(ss, val, ','); ++j) {\n",
        "            data[i][j] = stod(val);\n",
        "        }\n",
        "    }\n",
        "    return data;\n",
        "}\n",
        "\n",
        "// Load label CSV into 1D vector\n",
        "vector<int> load_csv_labels(const string& filename, int rows) {\n",
        "    vector<int> labels(rows);\n",
        "    ifstream file(filename);\n",
        "    string line;\n",
        "    for (int i = 0; i < rows && getline(file, line); ++i) {\n",
        "        labels[i] = stoi(line);\n",
        "    }\n",
        "    return labels;\n",
        "}\n",
        "\n",
        "double euclidean_distance(const vector<double>& a, const vector<double>& b) {\n",
        "    double sum = 0.0;\n",
        "    for (size_t i = 0; i < a.size(); ++i) {\n",
        "        double diff = a[i] - b[i];\n",
        "        sum += diff * diff;\n",
        "    }\n",
        "    return sqrt(sum);\n",
        "}\n",
        "\n",
        "int knn_predict(const vector<vector<double>>& X_train, const vector<int>& y_train,\n",
        "                const vector<double>& x_test, int k) {\n",
        "    vector<pair<double, int>> distances;\n",
        "    for (size_t i = 0; i < X_train.size(); ++i) {\n",
        "        double dist = euclidean_distance(X_train[i], x_test);\n",
        "        distances.push_back({dist, y_train[i]});\n",
        "    }\n",
        "\n",
        "    sort(distances.begin(), distances.end());\n",
        "\n",
        "    map<int, int> class_count;\n",
        "    for (int i = 0; i < k; ++i) {\n",
        "        class_count[distances[i].second]++;\n",
        "    }\n",
        "\n",
        "    int prediction = -1, max_count = -1;\n",
        "    for (const auto& pair : class_count) {\n",
        "        if (pair.second > max_count) {\n",
        "            max_count = pair.second;\n",
        "            prediction = pair.first;\n",
        "        }\n",
        "    }\n",
        "    return prediction;\n",
        "}\n",
        "\n",
        "void print_peak_ram_usage() {\n",
        "    struct rusage usage;\n",
        "    getrusage(RUSAGE_SELF, &usage);\n",
        "    cout << \"Peak RAM Usage: \" << usage.ru_maxrss / 1024.0 << \" MB\" << endl;\n",
        "}\n",
        "\n",
        "double estimate_bandwidth(const vector<vector<double>>& X_train, const vector<double>& x_test) {\n",
        "    auto start = high_resolution_clock::now();\n",
        "    for (size_t i = 0; i < X_train.size(); ++i) {\n",
        "        double sum = 0.0;\n",
        "        for (size_t j = 0; j < x_test.size(); ++j) {\n",
        "            double diff = X_train[i][j] - x_test[j];\n",
        "            sum += diff * diff;\n",
        "        }\n",
        "        volatile double result = sqrt(sum);\n",
        "    }\n",
        "    auto end = high_resolution_clock::now();\n",
        "    double elapsed = duration_cast<nanoseconds>(end - start).count() / 1e9;\n",
        "\n",
        "    size_t bytes = X_train.size() * x_test.size() * sizeof(double) * 2;\n",
        "    return bytes / elapsed / 1e9;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int num_train = 1000;\n",
        "    int num_test = 100;\n",
        "    int num_features = 784;\n",
        "    int k = 5;\n",
        "\n",
        "    vector<vector<double>> X_train = load_csv_data(\"X_train.csv\", num_train, num_features);\n",
        "    vector<int> y_train = load_csv_labels(\"y_train.csv\", num_train);\n",
        "    vector<vector<double>> X_test = load_csv_data(\"X_test.csv\", num_test, num_features);\n",
        "    vector<int> y_test = load_csv_labels(\"y_test.csv\", num_test);\n",
        "\n",
        "    vector<int> y_pred;\n",
        "    auto start = high_resolution_clock::now();\n",
        "\n",
        "    for (int i = 0; i < num_test; ++i) {\n",
        "        int pred = knn_predict(X_train, y_train, X_test[i], k);\n",
        "        y_pred.push_back(pred);\n",
        "    }\n",
        "\n",
        "    auto stop = high_resolution_clock::now();\n",
        "    auto duration = duration_cast<milliseconds>(stop - start);\n",
        "    int correct = 0;\n",
        "    for (int i = 0; i < num_test; ++i) {\n",
        "        if (y_pred[i] == y_test[i]) correct++;\n",
        "    }\n",
        "\n",
        "    cout << \"KNN-C++ Accuracy (MNIST): \" << static_cast<double>(correct) / num_test << endl;\n",
        "    cout << \"Execution Time: \" << duration.count() << \" ms\" << endl;\n",
        "    print_peak_ram_usage();\n",
        "    double bandwidth = estimate_bandwidth(X_train, X_test[0]);\n",
        "    cout << \"Estimated Memory Bandwidth: \" << bandwidth << \" GB/s\" << endl;\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxCUpaUpAse3",
        "outputId": "654d67d8-b4e4-4589-ff1b-5376a3e8b56c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing knn_cpu.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!g++ -O2 knn_cpu.cpp -o knn_cpu\n",
        "!./knn_cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9qHOjTeBQ31",
        "outputId": "2c26c2b1-4263-4c22-ac76-56a32dde990b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN-C++ Accuracy (MNIST): 0.96\n",
            "Execution Time: 124 ms\n",
            "Peak RAM Usage: 2745.22 MB\n",
            "Estimated Memory Bandwidth: 10.4639 GB/s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##MATRIX MULTIPLICATION"
      ],
      "metadata": {
        "id": "Ej64JTNQwY9c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Numpy"
      ],
      "metadata": {
        "id": "Sx-wsM7rqkpE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import tracemalloc\n",
        "import psutil\n",
        "import os\n",
        "\n",
        "def estimate_bandwidth(A, B, time_seconds):\n",
        "    bytes_read = A.nbytes + B.nbytes\n",
        "    gb_read = bytes_read / 1e9\n",
        "    return gb_read / time_seconds if time_seconds > 1e-6 else 0.0\n",
        "\n",
        "def benchmark_dense(n=1024):\n",
        "    print(\" Dense Matrix Multiplication (A @ B)\")\n",
        "\n",
        "    A = np.random.rand(n, n).astype(np.float32)\n",
        "    B = np.random.rand(n, n).astype(np.float32)\n",
        "\n",
        "    tracemalloc.start()\n",
        "    start_time = time.time()\n",
        "    C = A @ B\n",
        "    end_time = time.time()\n",
        "    _, peak = tracemalloc.get_traced_memory()\n",
        "    tracemalloc.stop()\n",
        "\n",
        "    exec_time = end_time - start_time\n",
        "    bandwidth = estimate_bandwidth(A, B, exec_time)\n",
        "\n",
        "    print(f\"Shape: ({n}, {n}) x ({n}, {n})\")\n",
        "    print(f\"Execution Time: {exec_time:.4f} seconds\")\n",
        "    print(f\"Peak RAM Usage: {peak / 1e6:.2f} MB\")\n",
        "    print(f\"Estimated Memory Bandwidth: {bandwidth:.2f} GB/s\")\n",
        "\n",
        "def benchmark_batched(batch_size=64, n=128):\n",
        "    print(\"\\n Batched Matrix Multiplication (np.matmul(A, B))\")\n",
        "\n",
        "    A = np.random.rand(batch_size, n, n).astype(np.float32)\n",
        "    B = np.random.rand(batch_size, n, n).astype(np.float32)\n",
        "\n",
        "    tracemalloc.start()\n",
        "    start_time = time.time()\n",
        "    C = np.matmul(A, B)  # or A @ B in NumPy 1.10+\n",
        "    end_time = time.time()\n",
        "    _, peak = tracemalloc.get_traced_memory()\n",
        "    tracemalloc.stop()\n",
        "\n",
        "    exec_time = end_time - start_time\n",
        "    bandwidth = estimate_bandwidth(A, B, exec_time)\n",
        "\n",
        "    print(f\"Shape: ({batch_size}, {n}, {n}) x ({batch_size}, {n}, {n})\")\n",
        "    print(f\"Execution Time: {exec_time:.4f} seconds\")\n",
        "    print(f\"Peak RAM Usage: {peak / 1e6:.2f} MB\")\n",
        "    print(f\"Estimated Memory Bandwidth: {bandwidth:.2f} GB/s\")\n",
        "\n",
        "def main():\n",
        "    benchmark_dense(n=1024)\n",
        "    benchmark_batched(batch_size=64, n=128)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tw-NcC6ewYAl",
        "outputId": "e75c6eba-ca11-40eb-85c0-a942c9948100"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Dense Matrix Multiplication (A @ B)\n",
            "Shape: (1024, 1024) x (1024, 1024)\n",
            "Execution Time: 0.0381 seconds\n",
            "Peak RAM Usage: 4.20 MB\n",
            "Estimated Memory Bandwidth: 0.22 GB/s\n",
            "\n",
            " Batched Matrix Multiplication (np.matmul(A, B))\n",
            "Shape: (64, 128, 128) x (64, 128, 128)\n",
            "Execution Time: 0.0078 seconds\n",
            "Peak RAM Usage: 4.20 MB\n",
            "Estimated Memory Bandwidth: 1.07 GB/s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####PyTorch"
      ],
      "metadata": {
        "id": "Exn36gX4zyrO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "import tracemalloc\n",
        "import psutil\n",
        "import os\n",
        "\n",
        "def estimate_bandwidth(A, B, time_seconds):\n",
        "    bytes_read = A.element_size() * (A.numel() + B.numel())\n",
        "    gb_read = bytes_read / 1e9\n",
        "    return gb_read / time_seconds if time_seconds > 1e-6 else 0.0\n",
        "\n",
        "def benchmark_dense(n=1024):\n",
        "    print(\" Dense Matrix Multiplication on CPU (A @ B)\")\n",
        "\n",
        "    A = torch.rand(n, n, dtype=torch.float32)\n",
        "    B = torch.rand(n, n, dtype=torch.float32)\n",
        "\n",
        "    tracemalloc.start()\n",
        "    start_time = time.time()\n",
        "    C = A @ B\n",
        "    end_time = time.time()\n",
        "    _, peak = tracemalloc.get_traced_memory()\n",
        "    tracemalloc.stop()\n",
        "\n",
        "    exec_time = end_time - start_time\n",
        "    bandwidth = estimate_bandwidth(A, B, exec_time)\n",
        "\n",
        "    print(f\"Shape: ({n}, {n}) x ({n}, {n})\")\n",
        "    print(f\"Execution Time: {exec_time:.4f} seconds\")\n",
        "    print(f\"Peak RAM Usage: {peak / 1e6:.2f} MB\")\n",
        "    print(f\"Estimated Memory Bandwidth: {bandwidth:.2f} GB/s\")\n",
        "\n",
        "def benchmark_batched(batch_size=64, n=128):\n",
        "    print(\"\\n Batched Matrix Multiplication on CPU (A @ B)\")\n",
        "\n",
        "    A = torch.rand(batch_size, n, n, dtype=torch.float32)\n",
        "    B = torch.rand(batch_size, n, n, dtype=torch.float32)\n",
        "\n",
        "    tracemalloc.start()\n",
        "    start_time = time.time()\n",
        "    C = torch.bmm(A, B)  # or torch.matmul\n",
        "    end_time = time.time()\n",
        "    _, peak = tracemalloc.get_traced_memory()\n",
        "    tracemalloc.stop()\n",
        "\n",
        "    exec_time = end_time - start_time\n",
        "    bandwidth = estimate_bandwidth(A, B, exec_time)\n",
        "\n",
        "    print(f\"Shape: ({batch_size}, {n}, {n}) x ({batch_size}, {n}, {n})\")\n",
        "    print(f\"Execution Time: {exec_time:.4f} seconds\")\n",
        "    print(f\"Peak RAM Usage: {peak / 1e6:.2f} MB\")\n",
        "    print(f\"Estimated Memory Bandwidth: {bandwidth:.2f} GB/s\")\n",
        "\n",
        "def main():\n",
        "    benchmark_dense(n=1024)\n",
        "    benchmark_batched(batch_size=64, n=128)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGXedqD1z1XT",
        "outputId": "c52c4868-8e40-4a74-d5e0-57a95849e5ec"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Dense Matrix Multiplication on CPU (A @ B)\n",
            "Shape: (1024, 1024) x (1024, 1024)\n",
            "Execution Time: 0.0610 seconds\n",
            "Peak RAM Usage: 0.00 MB\n",
            "Estimated Memory Bandwidth: 0.14 GB/s\n",
            "\n",
            " Batched Matrix Multiplication on CPU (A @ B)\n",
            "Shape: (64, 128, 128) x (64, 128, 128)\n",
            "Execution Time: 0.0091 seconds\n",
            "Peak RAM Usage: 0.00 MB\n",
            "Estimated Memory Bandwidth: 0.92 GB/s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####C++"
      ],
      "metadata": {
        "id": "4xCOinVn1kGN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile matrix_multiplication_cpp.cpp\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <chrono>\n",
        "#include <cstdlib>\n",
        "#include <sys/resource.h>\n",
        "#include <iomanip>\n",
        "\n",
        "using namespace std;\n",
        "using namespace chrono;\n",
        "\n",
        "long get_peak_ram_usage_mb() {\n",
        "    struct rusage usage;\n",
        "    getrusage(RUSAGE_SELF, &usage);\n",
        "    return usage.ru_maxrss / 1024;\n",
        "}\n",
        "\n",
        "void dense_matmul(const vector<vector<float>>& A,\n",
        "                  const vector<vector<float>>& B,\n",
        "                  vector<vector<float>>& C,\n",
        "                  int n) {\n",
        "    for (int i = 0; i < n; ++i)\n",
        "        for (int j = 0; j < n; ++j)\n",
        "            for (int k = 0; k < n; ++k)\n",
        "                C[i][j] += A[i][k] * B[k][j];\n",
        "}\n",
        "\n",
        "void batched_matmul(const vector<vector<vector<float>>>& A,\n",
        "                    const vector<vector<vector<float>>>& B,\n",
        "                    vector<vector<vector<float>>>& C,\n",
        "                    int batch, int n) {\n",
        "    for (int b = 0; b < batch; ++b)\n",
        "        for (int i = 0; i < n; ++i)\n",
        "            for (int j = 0; j < n; ++j)\n",
        "                for (int k = 0; k < n; ++k)\n",
        "                    C[b][i][j] += A[b][i][k] * B[b][k][j];\n",
        "}\n",
        "\n",
        "double estimate_bandwidth(long bytes, double seconds) {\n",
        "    return (bytes / 1e9) / seconds;\n",
        "}\n",
        "\n",
        "void run_dense(int n) {\n",
        "    cout << \" Dense Matrix Multiplication (C++ std::vector)\\n\";\n",
        "\n",
        "    vector<vector<float>> A(n, vector<float>(n));\n",
        "    vector<vector<float>> B(n, vector<float>(n));\n",
        "    vector<vector<float>> C(n, vector<float>(n, 0.0f));\n",
        "\n",
        "    for (auto& row : A)\n",
        "        for (auto& val : row) val = static_cast<float>(rand()) / RAND_MAX;\n",
        "    for (auto& row : B)\n",
        "        for (auto& val : row) val = static_cast<float>(rand()) / RAND_MAX;\n",
        "\n",
        "    long bytes = 2L * n * n * sizeof(float);\n",
        "\n",
        "    auto start = high_resolution_clock::now();\n",
        "    dense_matmul(A, B, C, n);\n",
        "    auto end = high_resolution_clock::now();\n",
        "\n",
        "    double duration = chrono::duration<double>(end - start).count();\n",
        "    double bandwidth = estimate_bandwidth(bytes, duration);\n",
        "    long peak_mem = get_peak_ram_usage_mb();\n",
        "\n",
        "    cout << fixed << setprecision(4);\n",
        "    cout << \"Size: \" << n << \" x \" << n << endl;\n",
        "    cout << \"Execution Time: \" << duration << \" seconds\\n\";\n",
        "    cout << \"Peak RAM Usage: \" << peak_mem << \" MB\\n\";\n",
        "    cout << \"Estimated Memory Bandwidth: \" << bandwidth << \" GB/s\\n\";\n",
        "}\n",
        "\n",
        "void run_batched(int batch, int n) {\n",
        "    cout << \"\\n Batched Matrix Multiplication (C++ std::vector)\\n\";\n",
        "\n",
        "    vector<vector<vector<float>>> A(batch, vector<vector<float>>(n, vector<float>(n)));\n",
        "    vector<vector<vector<float>>> B(batch, vector<vector<float>>(n, vector<float>(n)));\n",
        "    vector<vector<vector<float>>> C(batch, vector<vector<float>>(n, vector<float>(n, 0.0f)));\n",
        "\n",
        "    for (auto& mat : A)\n",
        "        for (auto& row : mat)\n",
        "            for (auto& val : row) val = static_cast<float>(rand()) / RAND_MAX;\n",
        "\n",
        "    for (auto& mat : B)\n",
        "        for (auto& row : mat)\n",
        "            for (auto& val : row) val = static_cast<float>(rand()) / RAND_MAX;\n",
        "\n",
        "    long bytes = 2L * batch * n * n * sizeof(float);\n",
        "\n",
        "    auto start = high_resolution_clock::now();\n",
        "    batched_matmul(A, B, C, batch, n);\n",
        "    auto end = high_resolution_clock::now();\n",
        "\n",
        "    double duration = chrono::duration<double>(end - start).count();\n",
        "    double bandwidth = estimate_bandwidth(bytes, duration);\n",
        "    long peak_mem = get_peak_ram_usage_mb();\n",
        "\n",
        "    cout << fixed << setprecision(4);\n",
        "    cout << \"Batch Size: \" << batch << \", Matrix Size: \" << n << \" x \" << n << endl;\n",
        "    cout << \"Execution Time: \" << duration << \" seconds\\n\";\n",
        "    cout << \"Peak RAM Usage: \" << peak_mem << \" MB\\n\";\n",
        "    cout << \"Estimated Memory Bandwidth: \" << bandwidth << \" GB/s\\n\";\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    srand(42);\n",
        "    run_dense(512);       // You can change to 1024 for larger benchmarks\n",
        "    run_batched(32, 64);  // You can increase if memory allows\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZtSDO-a2Z-G",
        "outputId": "2ca491df-fb3b-458b-81fb-f949f7843d33"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing matrix_multiplication_cpp.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!g++ -O2 -std=c++17 matrix_multiplication_cpp.cpp -o matmul_cpp\n"
      ],
      "metadata": {
        "id": "h7CEubBK15qb"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./matmul_cpp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ti-lvLZf2k64",
        "outputId": "4d937f7b-63fd-47dc-830f-93fe2339cd1e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Dense Matrix Multiplication (C++ std::vector)\n",
            "Size: 512 x 512\n",
            "Execution Time: 0.2166 seconds\n",
            "Peak RAM Usage: 2745 MB\n",
            "Estimated Memory Bandwidth: 0.0097 GB/s\n",
            "\n",
            " Batched Matrix Multiplication (C++ std::vector)\n",
            "Batch Size: 32, Matrix Size: 64 x 64\n",
            "Execution Time: 0.0099 seconds\n",
            "Peak RAM Usage: 2745 MB\n",
            "Estimated Memory Bandwidth: 0.1060 GB/s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##CNN\n",
        "\n",
        "Conv1: 1 → 16 channels, 3×3, ReLU\n",
        "\n",
        "MaxPool\n",
        "\n",
        "Conv2: 16 → 32 channels, 3×3, ReLU\n",
        "\n",
        "MaxPool\n",
        "\n",
        "Flatten\n",
        "\n",
        "FC1: 512 → 128\n",
        "\n",
        "FC2: 128 → 10 (class scores)"
      ],
      "metadata": {
        "id": "bZ6d4Xt09Ty5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Numpy"
      ],
      "metadata": {
        "id": "poThifZSrNX4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import tracemalloc\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def maxpool(x, size=2, stride=2):\n",
        "    n, c, h, w = x.shape\n",
        "    out_h = (h - size) // stride + 1\n",
        "    out_w = (w - size) // stride + 1\n",
        "    out = np.zeros((n, c, out_h, out_w))\n",
        "    for i in range(out_h):\n",
        "        for j in range(out_w):\n",
        "            x_slice = x[:, :, i*stride:i*stride+size, j*stride:j*stride+size]\n",
        "            out[:, :, i, j] = np.max(x_slice, axis=(2, 3))\n",
        "    return out\n",
        "\n",
        "def conv2d(x, w, b, stride=1, padding=0):\n",
        "    n, c_in, h, w_in = x.shape\n",
        "    c_out, _, k, _ = w.shape\n",
        "    h_out = (h + 2*padding - k) // stride + 1\n",
        "    w_out = (w_in + 2*padding - k) // stride + 1\n",
        "\n",
        "    x_padded = np.pad(x, ((0,0), (0,0), (padding, padding), (padding, padding)), mode='constant')\n",
        "    out = np.zeros((n, c_out, h_out, w_out))\n",
        "\n",
        "    for i in range(h_out):\n",
        "        for j in range(w_out):\n",
        "            x_slice = x_padded[:, :, i*stride:i*stride+k, j*stride:j*stride+k]\n",
        "            for f in range(c_out):\n",
        "                out[:, f, i, j] = np.sum(x_slice * w[f, :, :, :], axis=(1,2,3))\n",
        "    return out + b[None, :, None, None]\n",
        "\n",
        "def flatten(x):\n",
        "    return x.reshape(x.shape[0], -1)\n",
        "\n",
        "def fully_connected(x, w, b):\n",
        "    return np.dot(x, w.T) + b\n",
        "\n",
        "def estimate_bandwidth(arrays, time_seconds):\n",
        "    bytes_total = sum([a.nbytes for a in arrays])\n",
        "    return (bytes_total / 1e9) / time_seconds if time_seconds > 1e-6 else 0.0\n",
        "\n",
        "def main():\n",
        "    np.random.seed(42)\n",
        "    batch_size = 8\n",
        "    x = np.random.rand(batch_size, 1, 64, 64).astype(np.float32)\n",
        "\n",
        "    # Conv1: 1→16, kernel 3x3\n",
        "    w1 = np.random.rand(16, 1, 3, 3).astype(np.float32)\n",
        "    b1 = np.random.rand(16).astype(np.float32)\n",
        "\n",
        "    # Conv2: 16→32, kernel 3x3\n",
        "    w2 = np.random.rand(32, 16, 3, 3).astype(np.float32)\n",
        "    b2 = np.random.rand(32).astype(np.float32)\n",
        "\n",
        "    # 🛠 Fix: Compute correct input size after conv+pool layers\n",
        "    # After Conv1 → Pool → Conv2 → Pool: shape becomes (batch, 32, 16, 16)\n",
        "    fc1_in = 32 * 16 * 16  # = 8192\n",
        "    w_fc1 = np.random.rand(128, fc1_in).astype(np.float32)\n",
        "    b_fc1 = np.random.rand(128).astype(np.float32)\n",
        "\n",
        "    w_fc2 = np.random.rand(10, 128).astype(np.float32)\n",
        "    b_fc2 = np.random.rand(10).astype(np.float32)\n",
        "\n",
        "    # === Benchmark Start ===\n",
        "    tracemalloc.start()\n",
        "    start_time = time.time()\n",
        "\n",
        "    out = conv2d(x, w1, b1, stride=1, padding=1)\n",
        "    out = relu(out)\n",
        "    out = maxpool(out)\n",
        "\n",
        "    out = conv2d(out, w2, b2, stride=1, padding=1)\n",
        "    out = relu(out)\n",
        "    out = maxpool(out)\n",
        "\n",
        "    out = flatten(out)\n",
        "    out = relu(fully_connected(out, w_fc1, b_fc1))\n",
        "    out = fully_connected(out, w_fc2, b_fc2)\n",
        "\n",
        "    end_time = time.time()\n",
        "    _, peak = tracemalloc.get_traced_memory()\n",
        "    tracemalloc.stop()\n",
        "\n",
        "    exec_time = end_time - start_time\n",
        "    ram_mb = peak / 1e6\n",
        "    bandwidth = estimate_bandwidth([x, w1, w2, out], exec_time)\n",
        "\n",
        "    print(\" Large CNN - NumPy (CPU)\")\n",
        "    print(f\"Execution Time: {exec_time:.4f} sec\")\n",
        "    print(f\"Peak RAM Usage: {ram_mb:.2f} MB\")\n",
        "    print(f\"Estimated Memory Bandwidth: {bandwidth:.2f} GB/s\")\n",
        "    print(f\"Output shape: {out.shape}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJwR4rUH9__g",
        "outputId": "507581e5-dd80-4af4-f74a-a2b09aba2bbd"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Large CNN - NumPy (CPU)\n",
            "Execution Time: 6.4119 sec\n",
            "Peak RAM Usage: 8.94 MB\n",
            "Estimated Memory Bandwidth: 0.00 GB/s\n",
            "Output shape: (8, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####PyTorch"
      ],
      "metadata": {
        "id": "wXGIPMK8_pDS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import tracemalloc\n",
        "import psutil\n",
        "import os\n",
        "\n",
        "class CNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)   # (B, 1, 64, 64) → (B, 16, 64, 64)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)  # (B, 16, 32, 32) → (B, 32, 32, 32)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)         # halves H and W\n",
        "        self.fc1 = nn.Linear(32 * 16 * 16, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))  # → (B, 16, 32, 32)\n",
        "        x = self.pool(F.relu(self.conv2(x)))  # → (B, 32, 16, 16)\n",
        "        x = x.view(x.size(0), -1)             # → (B, 8192)\n",
        "        x = F.relu(self.fc1(x))               # → (B, 128)\n",
        "        x = self.fc2(x)                       # → (B, 10)\n",
        "        return x\n",
        "\n",
        "def estimate_bandwidth(tensors, exec_time):\n",
        "    total_bytes = sum([t.element_size() * t.numel() for t in tensors])\n",
        "    return (total_bytes / 1e9) / exec_time if exec_time > 1e-6 else 0.0\n",
        "\n",
        "def main():\n",
        "    torch.manual_seed(42)\n",
        "    device = torch.device(\"cpu\")\n",
        "    batch_size = 8\n",
        "\n",
        "    model = CNNModel().to(device)\n",
        "    x = torch.randn(batch_size, 1, 64, 64, device=device)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    tracemalloc.start()\n",
        "    start_time = time.time()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(x)\n",
        "\n",
        "    end_time = time.time()\n",
        "    _, peak = tracemalloc.get_traced_memory()\n",
        "    tracemalloc.stop()\n",
        "\n",
        "    exec_time = end_time - start_time\n",
        "    ram_mb = peak / 1e6\n",
        "    bandwidth = estimate_bandwidth([x, output], exec_time)\n",
        "\n",
        "    print(\" CNN - PyTorch (CPU)\")\n",
        "    print(f\"Execution Time: {exec_time:.4f} sec\")\n",
        "    print(f\"Peak RAM Usage: {ram_mb:.2f} MB\")\n",
        "    print(f\"Estimated Memory Bandwidth: {bandwidth:.2f} GB/s\")\n",
        "    print(f\"Output Shape: {output.shape}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAXQXCxN_rWY",
        "outputId": "5f2c9665-0de9-4751-9bb0-ccb26f995d09"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " CNN - PyTorch (CPU)\n",
            "Execution Time: 0.1256 sec\n",
            "Peak RAM Usage: 0.00 MB\n",
            "Estimated Memory Bandwidth: 0.00 GB/s\n",
            "Output Shape: torch.Size([8, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Comparing metrics by improving Metrics**\n",
        "\n",
        "Model   --   BatchSize\t --   Winner\n",
        "\n",
        "Small   \t--     (< 32)\t   --     CPU\n",
        "\n",
        "Medium   --     (64–128)\t --    Depends\n",
        "\n",
        "Large   -- \t   ( >128) --\t        GPU\n"
      ],
      "metadata": {
        "id": "eRmaGK0DA8Nb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Numpy with increased batch size"
      ],
      "metadata": {
        "id": "QPf0ElZqsJ1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "\n",
        "class CNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(32 * 16 * 16, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))  # → (B, 16, 32, 32)\n",
        "        x = self.pool(F.relu(self.conv2(x)))  # → (B, 32, 16, 16)\n",
        "        x = x.view(x.size(0), -1)             # → (B, 8192)\n",
        "        x = F.relu(self.fc1(x))               # → (B, 128)\n",
        "        x = self.fc2(x)                       # → (B, 10)\n",
        "        return x\n",
        "\n",
        "def estimate_bandwidth(tensors, exec_time):\n",
        "    total_bytes = sum([t.element_size() * t.numel() for t in tensors])\n",
        "    return (total_bytes / 1e9) / exec_time if exec_time > 1e-6 else 0.0\n",
        "\n",
        "def benchmark(device, batch_size):\n",
        "    print(f\"\\n Running on: {device.upper()} with batch size {batch_size}\")\n",
        "    torch.manual_seed(42)\n",
        "    model = CNNModel().to(device)\n",
        "    x = torch.randn(batch_size, 1, 64, 64, device=device)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Warm-up\n",
        "    with torch.no_grad():\n",
        "        _ = model(x)\n",
        "\n",
        "    if device == 'cuda':\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        output = model(x)\n",
        "    if device == 'cuda':\n",
        "        torch.cuda.synchronize()\n",
        "    end_time = time.time()\n",
        "\n",
        "    exec_time = end_time - start_time\n",
        "    mem = torch.cuda.max_memory_allocated() / 1e6 if device == 'cuda' else 0\n",
        "    bandwidth = estimate_bandwidth([x, output], exec_time)\n",
        "\n",
        "    print(f\"Execution Time: {exec_time:.4f} sec\")\n",
        "    print(f\"Peak {'GPU' if device=='cuda' else 'RAM'} Memory Usage: {mem:.2f} MB\")\n",
        "    print(f\"Estimated Memory Bandwidth: {bandwidth:.2f} GB/s\")\n",
        "    print(f\"Output Shape: {output.shape}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    batch_size = 128\n",
        "    benchmark('cpu', batch_size)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        benchmark('cuda', batch_size)\n",
        "    else:\n",
        "        print(\" GPU not available. Only CPU benchmark run.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQZK9zi7Bjz0",
        "outputId": "254f8bea-3a4c-4ce2-fae0-ed8d1ef18415"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Running on: CPU with batch size 128\n",
            "Execution Time: 0.1753 sec\n",
            "Peak RAM Memory Usage: 0.00 MB\n",
            "Estimated Memory Bandwidth: 0.01 GB/s\n",
            "Output Shape: torch.Size([128, 10])\n",
            " GPU not available. Only CPU benchmark run.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####PyTorch with increased batch size"
      ],
      "metadata": {
        "id": "oHbnbDs7sPb8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import tracemalloc\n",
        "\n",
        "class CNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(32 * 16 * 16, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return self.fc2(x)\n",
        "\n",
        "def estimate_bandwidth(tensors, time_sec):\n",
        "    total_bytes = sum([t.element_size() * t.numel() for t in tensors])\n",
        "    return (total_bytes / 1e9) / time_sec if time_sec > 0 else 0.0\n",
        "\n",
        "def main():\n",
        "    batch_size = 128\n",
        "    model = CNNModel().to(\"cpu\")\n",
        "    x = torch.randn(batch_size, 1, 64, 64)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _ = model(x)  # warm-up\n",
        "\n",
        "    tracemalloc.start()\n",
        "    start = time.time()\n",
        "    with torch.no_grad():\n",
        "        output = model(x)\n",
        "    end = time.time()\n",
        "    _, peak = tracemalloc.get_traced_memory()\n",
        "    tracemalloc.stop()\n",
        "\n",
        "    exec_time = end - start\n",
        "    ram_mb = peak / 1e6\n",
        "    bandwidth = estimate_bandwidth([x, output], exec_time)\n",
        "\n",
        "    print(\" CNN - PyTorch (CPU)\")\n",
        "    print(f\"Execution Time: {exec_time:.4f} sec\")\n",
        "    print(f\"Peak RAM Usage: {ram_mb:.2f} MB\")\n",
        "    print(f\"Estimated Memory Bandwidth: {bandwidth:.2f} GB/s\")\n",
        "    print(f\"Output Shape: {output.shape}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bV75zpNCPdQ",
        "outputId": "e1f49ace-3fb3-4ac4-a686-7cec160f0dda"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " CNN - PyTorch (CPU)\n",
            "Execution Time: 0.2223 sec\n",
            "Peak RAM Usage: 0.00 MB\n",
            "Estimated Memory Bandwidth: 0.01 GB/s\n",
            "Output Shape: torch.Size([128, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####C++ with increased batch size"
      ],
      "metadata": {
        "id": "ydq-IoEXDdpP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile cnn_forward_openmp.cpp\n",
        "\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <cmath>\n",
        "#include <chrono>\n",
        "#include <iomanip>\n",
        "#include <omp.h>\n",
        "\n",
        "using namespace std;\n",
        "using namespace chrono;\n",
        "\n",
        "typedef vector<vector<vector<vector<float>>>> Tensor4D;\n",
        "\n",
        "float relu(float x) { return x > 0 ? x : 0; }\n",
        "\n",
        "Tensor4D conv2d_omp(const Tensor4D& x, const Tensor4D& w, const vector<float>& b, int stride = 1, int padding = 0) {\n",
        "    int N = x.size(), C_in = x[0].size(), H = x[0][0].size(), W = x[0][0][0].size();\n",
        "    int C_out = w.size(), K = w[0][0].size();\n",
        "    int H_out = (H + 2 * padding - K) / stride + 1;\n",
        "    int W_out = (W + 2 * padding - K) / stride + 1;\n",
        "\n",
        "    Tensor4D out(N, vector<vector<vector<float>>>(C_out, vector<vector<float>>(H_out, vector<float>(W_out, 0.0f))));\n",
        "\n",
        "    #pragma omp parallel for collapse(4)\n",
        "    for (int n = 0; n < N; ++n)\n",
        "        for (int cout = 0; cout < C_out; ++cout)\n",
        "            for (int i = 0; i < H_out; ++i)\n",
        "                for (int j = 0; j < W_out; ++j) {\n",
        "                    float sum = 0.0;\n",
        "                    for (int cin = 0; cin < C_in; ++cin)\n",
        "                        for (int ki = 0; ki < K; ++ki)\n",
        "                            for (int kj = 0; kj < K; ++kj) {\n",
        "                                int row = i * stride + ki - padding;\n",
        "                                int col = j * stride + kj - padding;\n",
        "                                if (row >= 0 && col >= 0 && row < H && col < W)\n",
        "                                    sum += x[n][cin][row][col] * w[cout][cin][ki][kj];\n",
        "                            }\n",
        "                    out[n][cout][i][j] = sum + b[cout];\n",
        "                }\n",
        "    return out;\n",
        "}\n",
        "\n",
        "Tensor4D relu4D(const Tensor4D& x) {\n",
        "    Tensor4D out = x;\n",
        "    #pragma omp parallel for collapse(4)\n",
        "    for (int n = 0; n < x.size(); ++n)\n",
        "        for (int c = 0; c < x[0].size(); ++c)\n",
        "            for (int i = 0; i < x[0][0].size(); ++i)\n",
        "                for (int j = 0; j < x[0][0][0].size(); ++j)\n",
        "                    out[n][c][i][j] = relu(x[n][c][i][j]);\n",
        "    return out;\n",
        "}\n",
        "\n",
        "Tensor4D maxpool2D(const Tensor4D& x, int size = 2, int stride = 2) {\n",
        "    int N = x.size(), C = x[0].size(), H = x[0][0].size(), W = x[0][0][0].size();\n",
        "    int H_out = (H - size) / stride + 1;\n",
        "    int W_out = (W - size) / stride + 1;\n",
        "    Tensor4D out(N, vector<vector<vector<float>>>(C, vector<vector<float>>(H_out, vector<float>(W_out, 0.0f))));\n",
        "\n",
        "    #pragma omp parallel for collapse(4)\n",
        "    for (int n = 0; n < N; ++n)\n",
        "        for (int c = 0; c < C; ++c)\n",
        "            for (int i = 0; i < H_out; ++i)\n",
        "                for (int j = 0; j < W_out; ++j) {\n",
        "                    float max_val = -1e9;\n",
        "                    for (int ki = 0; ki < size; ++ki)\n",
        "                        for (int kj = 0; kj < size; ++kj)\n",
        "                            max_val = max(max_val, x[n][c][i * stride + ki][j * stride + kj]);\n",
        "                    out[n][c][i][j] = max_val;\n",
        "                }\n",
        "\n",
        "    return out;\n",
        "}\n",
        "\n",
        "vector<vector<float>> flatten(const Tensor4D& x) {\n",
        "    int N = x.size(), C = x[0].size(), H = x[0][0].size(), W = x[0][0][0].size();\n",
        "    vector<vector<float>> out(N, vector<float>(C * H * W));\n",
        "    #pragma omp parallel for collapse(2)\n",
        "    for (int n = 0; n < N; ++n)\n",
        "        for (int i = 0; i < C * H * W; ++i)\n",
        "            out[n][i] = x[n][i / (H * W)][(i / W) % H][i % W];\n",
        "    return out;\n",
        "}\n",
        "\n",
        "vector<vector<float>> dense(const vector<vector<float>>& x, const vector<vector<float>>& W, const vector<float>& b) {\n",
        "    int N = x.size(), D = W.size(), F = W[0].size();\n",
        "    vector<vector<float>> out(N, vector<float>(D, 0.0f));\n",
        "    #pragma omp parallel for collapse(2)\n",
        "    for (int n = 0; n < N; ++n)\n",
        "        for (int d = 0; d < D; ++d) {\n",
        "            float sum = 0;\n",
        "            for (int f = 0; f < F; ++f)\n",
        "                sum += x[n][f] * W[d][f];\n",
        "            out[n][d] = sum + b[d];\n",
        "        }\n",
        "    return out;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    const int B = 128;  // Bigger batch size\n",
        "    const int C = 1, H = 28, W = 28;\n",
        "    const int K1 = 3, K2 = 3;\n",
        "\n",
        "    Tensor4D input(B, vector<vector<vector<float>>>(C, vector<vector<float>>(H, vector<float>(W, 1.0f))));\n",
        "\n",
        "    Tensor4D w1(8, vector<vector<vector<float>>>(1, vector<vector<float>>(K1, vector<float>(K1, 0.1f))));\n",
        "    vector<float> b1(8, 0.1f);\n",
        "\n",
        "    Tensor4D w2(16, vector<vector<vector<float>>>(8, vector<vector<float>>(K2, vector<float>(K2, 0.1f))));\n",
        "    vector<float> b2(16, 0.1f);\n",
        "\n",
        "    vector<vector<float>> w_fc(10, vector<float>(16 * 5 * 5, 0.1f));\n",
        "    vector<float> b_fc(10, 0.1f);\n",
        "\n",
        "    cout << fixed << setprecision(4);\n",
        "    auto start = high_resolution_clock::now();\n",
        "\n",
        "    auto out1 = conv2d_omp(input, w1, b1, 1, 1);\n",
        "    auto act1 = relu4D(out1);\n",
        "    auto pool1 = maxpool2D(act1);\n",
        "\n",
        "    auto out2 = conv2d_omp(pool1, w2, b2, 1, 1);\n",
        "    auto act2 = relu4D(out2);\n",
        "    auto pool2 = maxpool2D(act2);\n",
        "\n",
        "    auto flat = flatten(pool2);\n",
        "    auto logits = dense(flat, w_fc, b_fc);\n",
        "\n",
        "    auto end = high_resolution_clock::now();\n",
        "    double duration = chrono::duration<double>(end - start).count();\n",
        "\n",
        "\n",
        "    cout << \"CNN Forward Pass (C++ OpenMP, Batch: \" << B << \")\\n\";\n",
        "    cout << \"Execution Time: \" << duration << \" seconds\\n\";\n",
        "    cout << \"Output Shape: (\" << logits.size() << \", \" << logits[0].size() << \")\\n\";\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwbnvsApDfbR",
        "outputId": "86d672cc-1455-4fcb-e3f5-a1dec57006e3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing cnn_forward_openmp.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!g++ -O2 -fopenmp cnn_forward_openmp.cpp -o cnn_openmp\n"
      ],
      "metadata": {
        "id": "azpLAj0JD02X"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./cnn_openmp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A77hzMYQEL44",
        "outputId": "03e59da2-257a-4ef6-90d2-c5622ec26350"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN Forward Pass (C++ OpenMP, Batch: 128)\n",
            "Execution Time: 0.1547 seconds\n",
            "Output Shape: (128, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Gradient Computation"
      ],
      "metadata": {
        "id": "eCIeI7ehseop"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Numpy"
      ],
      "metadata": {
        "id": "4aoQO5s3ynYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import psutil\n",
        "import tracemalloc\n",
        "import os\n",
        "\n",
        "def quadratic_function(x):\n",
        "    return np.sum(x ** 2)\n",
        "\n",
        "def compute_gradient_cpu(f, x, h=1e-5):\n",
        "    grad = np.zeros_like(x)\n",
        "    fx = f(x)\n",
        "    for i in range(x.size):\n",
        "        x_ph = x.copy()\n",
        "        x_mh = x.copy()\n",
        "        x_ph[i] += h\n",
        "        x_mh[i] -= h\n",
        "        grad[i] = (f(x_ph) - f(x_mh)) / (2 * h)\n",
        "    return grad\n",
        "\n",
        "def estimate_bandwidth(arrays, exec_time):\n",
        "    total_bytes = sum(a.nbytes for a in arrays)\n",
        "    return (total_bytes / 1e9) / exec_time if exec_time > 1e-6 else 0.0\n",
        "\n",
        "def main():\n",
        "    dim = 10_000\n",
        "    x = np.random.rand(dim).astype(np.float64)\n",
        "\n",
        "    # Warm-up\n",
        "    _ = compute_gradient_cpu(quadratic_function, x.copy())\n",
        "\n",
        "    # Start performance tracking\n",
        "    tracemalloc.start()\n",
        "    process = psutil.Process(os.getpid())\n",
        "    start_time = time.time()\n",
        "\n",
        "    grad = compute_gradient_cpu(quadratic_function, x.copy())\n",
        "\n",
        "    end_time = time.time()\n",
        "    _, peak = tracemalloc.get_traced_memory()\n",
        "    tracemalloc.stop()\n",
        "\n",
        "    exec_time = end_time - start_time\n",
        "    peak_ram = peak / 1e6  # MB\n",
        "    bandwidth = estimate_bandwidth([x, grad], exec_time)\n",
        "\n",
        "    print(\" CPU Gradient Computation - NumPy\")\n",
        "    print(f\"Input Dimension: {dim}\")\n",
        "    print(f\"Execution Time: {exec_time:.4f} sec\")\n",
        "    print(f\"Peak RAM Usage: {peak_ram:.2f} MB\")\n",
        "    print(f\"Estimated Memory Bandwidth: {bandwidth:.4f} GB/s\")\n",
        "    print(f\"Gradient Norm: {np.linalg.norm(grad):.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNyDpe09yqHe",
        "outputId": "edaf8878-b5d9-47c0-d3c5-ea6bb4b30cb6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " CPU Gradient Computation - NumPy\n",
            "Input Dimension: 10000\n",
            "Execution Time: 0.7264 sec\n",
            "Peak RAM Usage: 0.53 MB\n",
            "Estimated Memory Bandwidth: 0.0002 GB/s\n",
            "Gradient Norm: 115.0822\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Numpy\n",
        "**Increase the dimension**"
      ],
      "metadata": {
        "id": "oHnq9DOHs1T-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import psutil\n",
        "import tracemalloc\n",
        "import os\n",
        "\n",
        "def quadratic_function(x):\n",
        "    return np.sum(x ** 2)\n",
        "\n",
        "def compute_gradient_cpu(f, x, h=1e-5):\n",
        "    grad = np.zeros_like(x)\n",
        "    for i in range(x.size):\n",
        "        x_ph = x.copy()\n",
        "        x_mh = x.copy()\n",
        "        x_ph[i] += h\n",
        "        x_mh[i] -= h\n",
        "        grad[i] = (f(x_ph) - f(x_mh)) / (2 * h)\n",
        "    return grad\n",
        "\n",
        "def estimate_bandwidth(arrays, exec_time):\n",
        "    total_bytes = sum(a.nbytes for a in arrays)\n",
        "    return (total_bytes / 1e9) / exec_time if exec_time > 1e-6 else 0.0\n",
        "\n",
        "def main():\n",
        "    dim = 100_000\n",
        "    num_iters = 5\n",
        "    x = np.random.rand(dim).astype(np.float64)\n",
        "\n",
        "    tracemalloc.start()\n",
        "    start_time = time.time()\n",
        "\n",
        "    for _ in range(num_iters):\n",
        "        grad = compute_gradient_cpu(quadratic_function, x.copy())\n",
        "\n",
        "    end_time = time.time()\n",
        "    _, peak = tracemalloc.get_traced_memory()\n",
        "    tracemalloc.stop()\n",
        "\n",
        "    exec_time = end_time - start_time\n",
        "    peak_ram = peak / 1e6\n",
        "    bandwidth = estimate_bandwidth([x, grad], exec_time)\n",
        "\n",
        "    print(\" Scaled Gradient Computation - NumPy\")\n",
        "    print(f\"Input Dimension: {dim}\")\n",
        "    print(f\"Iterations: {num_iters}\")\n",
        "    print(f\"Execution Time: {exec_time:.4f} sec\")\n",
        "    print(f\"Peak RAM Usage: {peak_ram:.2f} MB\")\n",
        "    print(f\"Estimated Memory Bandwidth: {bandwidth:.4f} GB/s\")\n",
        "    print(f\"Gradient Norm: {np.linalg.norm(grad):.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5qmypuVzbUy",
        "outputId": "2d857d9c-9b65-4050-b29d-a238cca7b8a5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Scaled Gradient Computation - NumPy\n",
            "Input Dimension: 100000\n",
            "Iterations: 5\n",
            "Execution Time: 202.5222 sec\n",
            "Peak RAM Usage: 6.54 MB\n",
            "Estimated Memory Bandwidth: 0.0000 GB/s\n",
            "Gradient Norm: 364.5356\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####PyTorch"
      ],
      "metadata": {
        "id": "skimlklj3f9s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "import tracemalloc\n",
        "import psutil\n",
        "import os\n",
        "\n",
        "def quadratic_function(x):\n",
        "    return torch.sum(x ** 2)\n",
        "\n",
        "def compute_gradient_cpu(f, x, h=1e-5):\n",
        "    grad = torch.zeros_like(x)\n",
        "    for i in range(x.size(0)):\n",
        "        x_ph = x.clone()\n",
        "        x_mh = x.clone()\n",
        "        x_ph[i] += h\n",
        "        x_mh[i] -= h\n",
        "        grad[i] = (f(x_ph) - f(x_mh)) / (2 * h)\n",
        "    return grad\n",
        "\n",
        "def estimate_bandwidth(tensors, exec_time):\n",
        "    total_bytes = sum([t.element_size() * t.numel() for t in tensors])\n",
        "    return (total_bytes / 1e9) / exec_time if exec_time > 1e-6 else 0.0\n",
        "\n",
        "def main():\n",
        "    dim = 100_000\n",
        "    num_iters = 3\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "    x = torch.rand(dim, dtype=torch.float64, device=device)\n",
        "\n",
        "    tracemalloc.start()\n",
        "    process = psutil.Process(os.getpid())\n",
        "    start_time = time.time()\n",
        "\n",
        "    for _ in range(num_iters):\n",
        "        grad = compute_gradient_cpu(quadratic_function, x.clone())\n",
        "\n",
        "    end_time = time.time()\n",
        "    _, peak = tracemalloc.get_traced_memory()\n",
        "    tracemalloc.stop()\n",
        "\n",
        "    exec_time = end_time - start_time\n",
        "    peak_ram = peak / 1e6  # MB\n",
        "    bandwidth = estimate_bandwidth([x, grad], exec_time)\n",
        "\n",
        "    print(\" Gradient Computation - PyTorch (CPU)\")\n",
        "    print(f\"Input Dimension     : {dim}\")\n",
        "    print(f\"Iterations          : {num_iters}\")\n",
        "    print(f\"Execution Time      : {exec_time:.4f} sec\")\n",
        "    print(f\"Peak RAM Usage      : {peak_ram:.2f} MB\")\n",
        "    print(f\"Estimated Bandwidth : {bandwidth:.4f} GB/s\")\n",
        "    print(f\"Gradient Norm       : {torch.linalg.norm(grad):.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3wyvoN73Y8A",
        "outputId": "a29cd33a-c868-436e-fe74-938b5066de11"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Gradient Computation - PyTorch (CPU)\n",
            "Input Dimension     : 100000\n",
            "Iterations          : 3\n",
            "Execution Time      : 156.2450 sec\n",
            "Peak RAM Usage      : 0.17 MB\n",
            "Estimated Bandwidth : 0.0000 GB/s\n",
            "Gradient Norm       : 364.6636\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####C++"
      ],
      "metadata": {
        "id": "PVzngsZMtEmk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile gradient_cpu.cpp\n",
        "// Paste the full code here\n",
        "\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <cmath>\n",
        "#include <chrono>\n",
        "#include <numeric>\n",
        "\n",
        "using namespace std;\n",
        "using namespace chrono;\n",
        "\n",
        "double quadratic_function(const vector<double>& x) {\n",
        "    double sum = 0.0;\n",
        "    for (double val : x)\n",
        "        sum += val * val;\n",
        "    return sum;\n",
        "}\n",
        "\n",
        "vector<double> compute_gradient_cpu(double (*f)(const vector<double>&), const vector<double>& x, double h = 1e-5) {\n",
        "    int n = x.size();\n",
        "    vector<double> grad(n);\n",
        "    for (int i = 0; i < n; ++i) {\n",
        "        vector<double> x_ph = x;\n",
        "        vector<double> x_mh = x;\n",
        "        x_ph[i] += h;\n",
        "        x_mh[i] -= h;\n",
        "        grad[i] = (f(x_ph) - f(x_mh)) / (2 * h);\n",
        "    }\n",
        "    return grad;\n",
        "}\n",
        "\n",
        "double estimate_bandwidth(const vector<double>& x, const vector<double>& grad, double time_sec) {\n",
        "    size_t bytes = (x.size() + grad.size()) * sizeof(double);\n",
        "    return (bytes / 1e9) / time_sec;\n",
        "}\n",
        "\n",
        "double norm(const vector<double>& v) {\n",
        "    double sum_sq = 0.0;\n",
        "    for (double val : v)\n",
        "        sum_sq += val * val;\n",
        "    return sqrt(sum_sq);\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int dim = 100000;\n",
        "    int iters = 3;\n",
        "    vector<double> x(dim);\n",
        "\n",
        "    for (int i = 0; i < dim; ++i)\n",
        "        x[i] = static_cast<double>(rand()) / RAND_MAX;\n",
        "\n",
        "    auto start = high_resolution_clock::now();\n",
        "\n",
        "    vector<double> grad;\n",
        "    for (int i = 0; i < iters; ++i)\n",
        "        grad = compute_gradient_cpu(quadratic_function, x);\n",
        "\n",
        "    auto end = high_resolution_clock::now();\n",
        "    double elapsed = duration_cast<duration<double>>(end - start).count();\n",
        "    double bandwidth = estimate_bandwidth(x, grad, elapsed);\n",
        "\n",
        "    cout << \" Gradient Computation - C++ (CPU)\" << endl;\n",
        "    cout << \"Input Dimension     : \" << dim << endl;\n",
        "    cout << \"Iterations          : \" << iters << endl;\n",
        "    cout << \"Execution Time      : \" << elapsed << \" sec\" << endl;\n",
        "    cout << \"Estimated Bandwidth : \" << bandwidth << \" GB/s\" << endl;\n",
        "    cout << \"Gradient Norm       : \" << norm(grad) << endl;\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7N9HYvX5goZ",
        "outputId": "0a001481-9f15-4c3e-fb18-b7a879c8273a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing gradient_cpu.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!g++ -O2 gradient_cpu.cpp -o gradient_cpu"
      ],
      "metadata": {
        "id": "WS5HDMcW5rBx"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./gradient_cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eW2Yi9Cx5vcS",
        "outputId": "95323f33-a7b9-45e5-e19a-54ba57268d87"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Gradient Computation - C++ (CPU)\n",
            "Input Dimension     : 100000\n",
            "Iterations          : 3\n",
            "Execution Time      : 332.439 sec\n",
            "Estimated Bandwidth : 4.81292e-06 GB/s\n",
            "Gradient Norm       : 365.052\n"
          ]
        }
      ]
    }
  ]
}