{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##KNN"
      ],
      "metadata": {
        "id": "dFJEuJpQykNy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Numpy (GPU accelerated)"
      ],
      "metadata": {
        "id": "qkY2zTZ7J142"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cupy as cp\n",
        "import time\n",
        "import os\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def knn_cupy(X_train, y_train, X_test, k=3):\n",
        "    predictions = []\n",
        "    for test_point in X_test:\n",
        "        distances = cp.linalg.norm(X_train - test_point, axis=1)\n",
        "        nearest = cp.argsort(distances)[:k]\n",
        "        top_k_labels = y_train[nearest]\n",
        "        predicted = cp.bincount(top_k_labels).argmax()\n",
        "        predictions.append(predicted)\n",
        "    return cp.array(predictions)\n",
        "\n",
        "\n",
        "def estimate_bandwidth(X_train, X_test, time_seconds):\n",
        "    bytes_read = X_train.nbytes + X_test.nbytes\n",
        "    gb_read = bytes_read / 1e9\n",
        "    return gb_read / time_seconds if time_seconds > 0 else 0\n",
        "\n",
        "def main():\n",
        "    # Load and split data (on CPU)\n",
        "    X, y = load_digits(return_X_y=True)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Move data to GPU\n",
        "    X_train_gpu = cp.asarray(X_train)\n",
        "    X_test_gpu = cp.asarray(X_test)\n",
        "    y_train_gpu = cp.asarray(y_train)\n",
        "\n",
        "    # Track memory before\n",
        "    mem_before, _ = cp.cuda.Device().mem_info\n",
        "\n",
        "    # Time the KNN\n",
        "    start = time.time()\n",
        "    y_pred_gpu = knn_cupy(X_train_gpu, y_train_gpu, X_test_gpu, k=3)\n",
        "    cp.cuda.Device().synchronize()\n",
        "    end = time.time()\n",
        "\n",
        "    # Convert predictions back to NumPy for accuracy check\n",
        "    y_pred = cp.asnumpy(y_pred_gpu)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    # Track memory after\n",
        "    mem_after, _ = cp.cuda.Device().mem_info\n",
        "    peak_used = (mem_before - mem_after) / 1e6  # in MB\n",
        "\n",
        "    # Bandwidth\n",
        "    exec_time = end - start\n",
        "    bandwidth = estimate_bandwidth(X_train_gpu, X_test_gpu, exec_time)\n",
        "\n",
        "    print(f\"KNN-CuPy Accuracy: {acc:.2f}\")\n",
        "    print(f\"Execution Time: {exec_time:.4f} seconds\")\n",
        "    print(f\"Peak GPU Memory Usage: {peak_used:.2f} MB\")\n",
        "    print(f\"Estimated Memory Bandwidth: {bandwidth:.2f} GB/s\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OphtJXcujVww",
        "outputId": "c499fb68-fd75-4c0a-e8ea-f0de4cbf5377"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN-CuPy Accuracy: 0.98\n",
            "Execution Time: 9.4581 seconds\n",
            "Peak GPU Memory Usage: 4.19 MB\n",
            "Estimated Memory Bandwidth: 0.00 GB/s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####KNN for large data"
      ],
      "metadata": {
        "id": "pfIgBaNLJ9Vl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cupy as cp\n",
        "import time\n",
        "import tracemalloc\n",
        "import psutil\n",
        "import os\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def knn_numpy(X_train, y_train, X_test, k=3):\n",
        "    predictions = []\n",
        "    for test_point in X_test:\n",
        "        distances = np.linalg.norm(X_train - test_point, axis=1)\n",
        "        nearest = np.argsort(distances)[:k]\n",
        "        top_k_labels = y_train[nearest]\n",
        "        predicted = np.bincount(top_k_labels).argmax()\n",
        "        predictions.append(predicted)\n",
        "    return np.array(predictions)\n",
        "\n",
        "def knn_cupy(X_train, y_train, X_test, k=3):\n",
        "    predictions = []\n",
        "    for test_point in X_test:\n",
        "        distances = cp.linalg.norm(X_train - test_point, axis=1)\n",
        "        nearest = cp.argsort(distances)[:k]\n",
        "        top_k_labels = y_train[nearest]\n",
        "        predicted = cp.bincount(top_k_labels).argmax()\n",
        "        predictions.append(predicted)\n",
        "    return cp.array(predictions)\n",
        "\n",
        "def estimate_bandwidth(X_train, X_test, time_seconds):\n",
        "    total_bytes = X_train.nbytes + X_test.nbytes\n",
        "    return (total_bytes / 1e9) / time_seconds if time_seconds > 0 else 0\n",
        "\n",
        "def benchmark_knn_numpy(X_train, y_train, X_test, y_test):\n",
        "    tracemalloc.start()\n",
        "    start = time.time()\n",
        "    y_pred = knn_numpy(X_train, y_train, X_test, k=3)\n",
        "    end = time.time()\n",
        "    _, peak = tracemalloc.get_traced_memory()\n",
        "    peak_mb = peak / 1e6\n",
        "    tracemalloc.stop()\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    bandwidth = estimate_bandwidth(X_train, X_test, end - start)\n",
        "    print(\"\\n KNN - NumPy (CPU)\")\n",
        "    print(f\"Accuracy: {acc:.2f}\")\n",
        "    print(f\"Execution Time: {end - start:.4f} seconds\")\n",
        "    print(f\"Peak RAM Usage: {peak_mb:.2f} MB\")\n",
        "    print(f\"Estimated Memory Bandwidth: {bandwidth:.2f} GB/s\")\n",
        "\n",
        "def benchmark_knn_cupy(X_train, y_train, X_test, y_test):\n",
        "    # Move data to GPU\n",
        "    X_train_gpu = cp.asarray(X_train)\n",
        "    X_test_gpu = cp.asarray(X_test)\n",
        "    y_train_gpu = cp.asarray(y_train)\n",
        "\n",
        "    mem_before, _ = cp.cuda.Device().mem_info\n",
        "    start = time.time()\n",
        "    y_pred_gpu = knn_cupy(X_train_gpu, y_train_gpu, X_test_gpu, k=3)\n",
        "    cp.cuda.Device().synchronize()\n",
        "    end = time.time()\n",
        "    mem_after, _ = cp.cuda.Device().mem_info\n",
        "    peak_used = (mem_before - mem_after) / 1e6\n",
        "    acc = accuracy_score(y_test, cp.asnumpy(y_pred_gpu))\n",
        "    bandwidth = estimate_bandwidth(X_train_gpu, X_test_gpu, end - start)\n",
        "    print(\"\\n KNN - CuPy (GPU)\")\n",
        "    print(f\"Accuracy: {acc:.2f}\")\n",
        "    print(f\"Execution Time: {end - start:.4f} seconds\")\n",
        "    print(f\"Peak GPU Memory Usage: {peak_used:.2f} MB\")\n",
        "    print(f\"Estimated Memory Bandwidth: {bandwidth:.2f} GB/s\")\n",
        "\n",
        "def main():\n",
        "    print(\" Loading MNIST dataset...\")\n",
        "    mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
        "    X, y = mnist.data, mnist.target.astype(int)\n",
        "\n",
        "    # Normalize and sample a smaller subset for speed\n",
        "    X = X / 255.0\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1000, train_size=5000, random_state=42)\n",
        "\n",
        "    benchmark_knn_numpy(X_train, y_train, X_test, y_test)\n",
        "    benchmark_knn_cupy(X_train, y_train, X_test, y_test)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLgW4BkXnK-O",
        "outputId": "e3ce1ffb-0297-472e-bcf9-4b5ccc451bea"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Loading MNIST dataset...\n",
            "\n",
            " KNN - NumPy (CPU)\n",
            "Accuracy: 0.93\n",
            "Execution Time: 14.2214 seconds\n",
            "Peak RAM Usage: 94.29 MB\n",
            "Estimated Memory Bandwidth: 0.00 GB/s\n",
            "\n",
            " KNN - CuPy (GPU)\n",
            "Accuracy: 0.93\n",
            "Execution Time: 1.4518 seconds\n",
            "Peak GPU Memory Usage: 31.46 MB\n",
            "Estimated Memory Bandwidth: 0.03 GB/s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####pytorch for MNIST"
      ],
      "metadata": {
        "id": "6pJsIKUjrslT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "import tracemalloc\n",
        "import psutil\n",
        "import os\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def knn_torch(X_train, y_train, X_test, k=3):\n",
        "    predictions = []\n",
        "    for test_point in X_test:\n",
        "        distances = torch.norm(X_train - test_point, dim=1)\n",
        "        nearest = torch.topk(distances, k, largest=False).indices\n",
        "        top_k_labels = y_train[nearest]\n",
        "        predicted = torch.mode(top_k_labels).values.item()\n",
        "        predictions.append(predicted)\n",
        "    return torch.tensor(predictions)\n",
        "\n",
        "def estimate_bandwidth(X_train, X_test, time_seconds):\n",
        "    bytes_read = X_train.element_size() * (X_train.numel() + X_test.numel())\n",
        "    gb_read = bytes_read / 1e9\n",
        "    return gb_read / time_seconds if time_seconds > 1e-6 else 0.0\n",
        "\n",
        "def benchmark_knn_torch(X_train_np, y_train_np, X_test_np, y_test_np, device):\n",
        "    print(f\"\\n KNN - PyTorch ({device.upper()})\")\n",
        "\n",
        "    # Move to device\n",
        "    X_train = torch.tensor(X_train_np, dtype=torch.float32, device=device)\n",
        "    X_test = torch.tensor(X_test_np, dtype=torch.float32, device=device)\n",
        "    y_train = torch.tensor(y_train_np, dtype=torch.int64, device=device)\n",
        "\n",
        "    if device == 'cuda':\n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "    tracemalloc.start()\n",
        "    start_time = time.time()\n",
        "    y_pred = knn_torch(X_train, y_train, X_test, k=3)\n",
        "    if device == 'cuda':\n",
        "        torch.cuda.synchronize()\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Accuracy\n",
        "    y_pred_np = y_pred.cpu().numpy()\n",
        "    acc = accuracy_score(y_test_np, y_pred_np)\n",
        "\n",
        "    # Memory usage\n",
        "    _, peak = tracemalloc.get_traced_memory()\n",
        "    peak_mb = peak / 1e6\n",
        "\n",
        "    # GPU memory (if applicable)\n",
        "    gpu_mem = torch.cuda.max_memory_allocated() / 1e6 if device == 'cuda' else 0\n",
        "\n",
        "    # Bandwidth\n",
        "    exec_time = end_time - start_time\n",
        "    bandwidth = estimate_bandwidth(X_train, X_test, exec_time)\n",
        "\n",
        "    # Output\n",
        "    print(f\"Accuracy: {acc:.2f}\")\n",
        "    print(f\"Execution Time: {exec_time:.4f} seconds\")\n",
        "    print(f\"Peak RAM Usage: {peak_mb:.2f} MB\")\n",
        "    if device == 'cuda':\n",
        "        print(f\"Peak GPU Memory Usage: {gpu_mem:.2f} MB\")\n",
        "    print(f\"Estimated Memory Bandwidth: {bandwidth:.2f} GB/s\")\n",
        "\n",
        "def main():\n",
        "    print(\" Loading MNIST dataset...\")\n",
        "    mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
        "    X, y = mnist.data, mnist.target.astype(int)\n",
        "    X = X.astype(\"float32\") / 255.0\n",
        "\n",
        "    # Use a subset for performance\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=5000, test_size=1000, random_state=42)\n",
        "\n",
        "    benchmark_knn_torch(X_train, y_train, X_test, y_test, device='cpu')\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        benchmark_knn_torch(X_train, y_train, X_test, y_test, device='cuda')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8t_3V59mriPl",
        "outputId": "1a30654d-2451-40ca-b0f9-2724e3ffeedb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Loading MNIST dataset...\n",
            "\n",
            " KNN - PyTorch (CPU)\n",
            "Accuracy: 0.93\n",
            "Execution Time: 3.3407 seconds\n",
            "Peak RAM Usage: 0.16 MB\n",
            "Estimated Memory Bandwidth: 0.01 GB/s\n",
            "\n",
            " KNN - PyTorch (CUDA)\n",
            "Accuracy: 0.93\n",
            "Execution Time: 0.6047 seconds\n",
            "Peak RAM Usage: 0.17 MB\n",
            "Peak GPU Memory Usage: 34.58 MB\n",
            "Estimated Memory Bandwidth: 0.03 GB/s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####C++ (GPU accelerated)"
      ],
      "metadata": {
        "id": "Shb7mXBXKO-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile knn_gpu.cu\n",
        "#include <iostream>\n",
        "#include <fstream>\n",
        "#include <sstream>\n",
        "#include <vector>\n",
        "#include <cmath>\n",
        "#include <map>\n",
        "#include <algorithm>\n",
        "#include <chrono>\n",
        "#include <sys/resource.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "using namespace std;\n",
        "using namespace chrono;\n",
        "\n",
        "#define BLOCK_SIZE 256\n",
        "\n",
        "__global__ void compute_distances(const double* X_train, const double* x_test, double* distances, int num_train, int num_features) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < num_train) {\n",
        "        double sum = 0.0;\n",
        "        for (int j = 0; j < num_features; ++j) {\n",
        "            double diff = X_train[idx * num_features + j] - x_test[j];\n",
        "            sum += diff * diff;\n",
        "        }\n",
        "        distances[idx] = sqrt(sum);\n",
        "    }\n",
        "}\n",
        "\n",
        "// Utility to load CSV\n",
        "vector<vector<double>> load_csv_data(const string& filename, int rows, int cols) {\n",
        "    vector<vector<double>> data(rows, vector<double>(cols));\n",
        "    ifstream file(filename);\n",
        "    string line;\n",
        "    for (int i = 0; i < rows && getline(file, line); ++i) {\n",
        "        stringstream ss(line);\n",
        "        string val;\n",
        "        for (int j = 0; j < cols && getline(ss, val, ','); ++j) {\n",
        "            data[i][j] = stod(val);\n",
        "        }\n",
        "    }\n",
        "    return data;\n",
        "}\n",
        "\n",
        "vector<int> load_csv_labels(const string& filename, int rows) {\n",
        "    vector<int> labels(rows);\n",
        "    ifstream file(filename);\n",
        "    string line;\n",
        "    for (int i = 0; i < rows && getline(file, line); ++i) {\n",
        "        labels[i] = stoi(line);\n",
        "    }\n",
        "    return labels;\n",
        "}\n",
        "\n",
        "// KNN classifier using GPU distances\n",
        "int knn_gpu_predict(const vector<vector<double>>& X_train, const vector<int>& y_train,\n",
        "                    const vector<double>& x_test, int k, int num_features) {\n",
        "    int num_train = X_train.size();\n",
        "\n",
        "    double* d_X_train;\n",
        "    double* d_x_test;\n",
        "    double* d_distances;\n",
        "    cudaMalloc(&d_X_train, num_train * num_features * sizeof(double));\n",
        "    cudaMalloc(&d_x_test, num_features * sizeof(double));\n",
        "    cudaMalloc(&d_distances, num_train * sizeof(double));\n",
        "\n",
        "    // Flatten train\n",
        "    vector<double> X_train_flat(num_train * num_features);\n",
        "    for (int i = 0; i < num_train; ++i)\n",
        "        for (int j = 0; j < num_features; ++j)\n",
        "            X_train_flat[i * num_features + j] = X_train[i][j];\n",
        "\n",
        "    cudaMemcpy(d_X_train, X_train_flat.data(), num_train * num_features * sizeof(double), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_x_test, x_test.data(), num_features * sizeof(double), cudaMemcpyHostToDevice);\n",
        "\n",
        "    int threads = BLOCK_SIZE;\n",
        "    int blocks = (num_train + threads - 1) / threads;\n",
        "    compute_distances<<<blocks, threads>>>(d_X_train, d_x_test, d_distances, num_train, num_features);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    vector<double> distances(num_train);\n",
        "    cudaMemcpy(distances.data(), d_distances, num_train * sizeof(double), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    vector<pair<double, int>> dist_label;\n",
        "    for (int i = 0; i < num_train; ++i)\n",
        "        dist_label.push_back({distances[i], y_train[i]});\n",
        "    sort(dist_label.begin(), dist_label.end());\n",
        "\n",
        "    map<int, int> count;\n",
        "    for (int i = 0; i < k; ++i)\n",
        "        count[dist_label[i].second]++;\n",
        "\n",
        "    int best_label = -1, max_votes = -1;\n",
        "    for (auto& [label, votes] : count) {\n",
        "        if (votes > max_votes) {\n",
        "            best_label = label;\n",
        "            max_votes = votes;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    cudaFree(d_X_train);\n",
        "    cudaFree(d_x_test);\n",
        "    cudaFree(d_distances);\n",
        "    return best_label;\n",
        "}\n",
        "\n",
        "void print_peak_ram_usage() {\n",
        "    struct rusage usage;\n",
        "    getrusage(RUSAGE_SELF, &usage);\n",
        "    cout << \"Peak RAM Usage: \" << usage.ru_maxrss / 1024.0 << \" MB\" << endl;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int num_train = 1000;\n",
        "    int num_test = 100;\n",
        "    int num_features = 784;\n",
        "    int k = 5;\n",
        "\n",
        "    auto X_train = load_csv_data(\"X_train.csv\", num_train, num_features);\n",
        "    auto y_train = load_csv_labels(\"y_train.csv\", num_train);\n",
        "    auto X_test = load_csv_data(\"X_test.csv\", num_test, num_features);\n",
        "    auto y_test = load_csv_labels(\"y_test.csv\", num_test);\n",
        "\n",
        "    vector<int> y_pred;\n",
        "\n",
        "    auto start = high_resolution_clock::now();\n",
        "\n",
        "    for (int i = 0; i < num_test; ++i) {\n",
        "        int pred = knn_gpu_predict(X_train, y_train, X_test[i], k, num_features);\n",
        "        y_pred.push_back(pred);\n",
        "    }\n",
        "\n",
        "    auto stop = high_resolution_clock::now();\n",
        "    auto duration = duration_cast<milliseconds>(stop - start).count();\n",
        "    int correct = 0;\n",
        "    for (int i = 0; i < num_test; ++i) {\n",
        "        if (y_pred[i] == y_test[i]) correct++;\n",
        "    }\n",
        "\n",
        "    cout << \"KNN-CUDA Accuracy: \" << static_cast<double>(correct) / num_test << endl;\n",
        "    cout << \"Execution Time: \" << duration << \" ms\" << endl;\n",
        "    print_peak_ram_usage();\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgszWi2RIcnf",
        "outputId": "f9dd2789-541b-4dcc-d735-25d2f8b04dee"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing knn_gpu.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -O2 knn_gpu.cu -o knn_gpu"
      ],
      "metadata": {
        "id": "VRuOqJr3InUJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./knn_gpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3Kz8EG6JPmh",
        "outputId": "41c29975-dd60-4d20-f2ae-bee02b50dbd4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN-CUDA Accuracy: 1\n",
            "Execution Time: 476 ms\n",
            "Peak RAM Usage: 2190.63 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Matrix multiplication"
      ],
      "metadata": {
        "id": "FUjo3dogKXeA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Numpy (GPU accelerated)"
      ],
      "metadata": {
        "id": "MtGP4lcUy6K_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cupy as cp\n",
        "import time\n",
        "\n",
        "def estimate_bandwidth(A, B, time_seconds):\n",
        "    bytes_read = A.nbytes + B.nbytes\n",
        "    gb_read = bytes_read / 1e9\n",
        "    return gb_read / time_seconds if time_seconds > 1e-6 else 0.0\n",
        "\n",
        "def benchmark_dense(n=1024):\n",
        "    print(\" Dense Matrix Multiplication on GPU (A @ B)\")\n",
        "\n",
        "    A = cp.random.rand(n, n, dtype=cp.float32)\n",
        "    B = cp.random.rand(n, n, dtype=cp.float32)\n",
        "\n",
        "    cp.cuda.Device().synchronize()\n",
        "    mem_before = cp.cuda.Device().mem_info[0]\n",
        "\n",
        "    start_time = time.time()\n",
        "    C = A @ B\n",
        "    cp.cuda.Device().synchronize()\n",
        "    end_time = time.time()\n",
        "\n",
        "    mem_after = cp.cuda.Device().mem_info[0]\n",
        "    peak_mem = (mem_before - mem_after) / 1e6  # MB\n",
        "\n",
        "    exec_time = end_time - start_time\n",
        "    bandwidth = estimate_bandwidth(A, B, exec_time)\n",
        "\n",
        "    print(f\"Shape: ({n}, {n}) x ({n}, {n})\")\n",
        "    print(f\"Execution Time: {exec_time:.4f} seconds\")\n",
        "    print(f\"Approx. Peak GPU Memory Usage: {peak_mem:.2f} MB\")\n",
        "    print(f\"Estimated Memory Bandwidth: {bandwidth:.2f} GB/s\")\n",
        "\n",
        "\n",
        "def benchmark_batched(batch_size=64, n=128):\n",
        "    print(\"\\n Batched Matrix Multiplication on GPU (A @ B)\")\n",
        "\n",
        "    A = cp.random.rand(batch_size, n, n, dtype=cp.float32)\n",
        "    B = cp.random.rand(batch_size, n, n, dtype=cp.float32)\n",
        "\n",
        "    cp.cuda.Device().synchronize()\n",
        "    mem_before = cp.cuda.Device().mem_info[0]\n",
        "\n",
        "    start_time = time.time()\n",
        "    C = cp.matmul(A, B)\n",
        "    cp.cuda.Device().synchronize()\n",
        "    end_time = time.time()\n",
        "\n",
        "    mem_after = cp.cuda.Device().mem_info[0]\n",
        "    peak_mem = (mem_before - mem_after) / 1e6  # in MB\n",
        "\n",
        "    exec_time = end_time - start_time\n",
        "    bandwidth = estimate_bandwidth(A, B, exec_time)\n",
        "\n",
        "    print(f\"Shape: ({batch_size}, {n}, {n}) x ({batch_size}, {n}, {n})\")\n",
        "    print(f\"Execution Time: {exec_time:.4f} seconds\")\n",
        "    print(f\"Approx. Peak GPU Memory Usage: {peak_mem:.2f} MB\")\n",
        "    print(f\"Estimated Memory Bandwidth: {bandwidth:.2f} GB/s\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    benchmark_dense(n=1024)\n",
        "    benchmark_batched(batch_size=64, n=128)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdDMc1B3w5s3",
        "outputId": "c1eed6e1-7a81-45b8-929a-1d6a6d5f2476"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Dense Matrix Multiplication on GPU (A @ B)\n",
            "Shape: (1024, 1024) x (1024, 1024)\n",
            "Execution Time: 0.1173 seconds\n",
            "Approx. Peak GPU Memory Usage: 8.39 MB\n",
            "Estimated Memory Bandwidth: 0.07 GB/s\n",
            "\n",
            " Batched Matrix Multiplication on GPU (A @ B)\n",
            "Shape: (64, 128, 128) x (64, 128, 128)\n",
            "Execution Time: 0.0016 seconds\n",
            "Approx. Peak GPU Memory Usage: 0.00 MB\n",
            "Estimated Memory Bandwidth: 5.30 GB/s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "####PyTorch - GPU"
      ],
      "metadata": {
        "id": "3kIS_HeH0aDR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "\n",
        "def estimate_bandwidth(A, B, time_seconds):\n",
        "    bytes_read = A.element_size() * (A.numel() + B.numel())\n",
        "    gb_read = bytes_read / 1e9\n",
        "    return gb_read / time_seconds if time_seconds > 1e-6 else 0.0\n",
        "\n",
        "def benchmark_dense(n=1024):\n",
        "    print(\" Dense Matrix Multiplication on GPU (A @ B)\")\n",
        "\n",
        "    A = torch.rand(n, n, dtype=torch.float32, device='cuda')\n",
        "    B = torch.rand(n, n, dtype=torch.float32, device='cuda')\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "    start_time = time.time()\n",
        "    C = A @ B\n",
        "    torch.cuda.synchronize()\n",
        "    end_time = time.time()\n",
        "\n",
        "    exec_time = end_time - start_time\n",
        "    peak_mem = torch.cuda.max_memory_allocated() / 1e6  # MB\n",
        "    bandwidth = estimate_bandwidth(A, B, exec_time)\n",
        "\n",
        "    print(f\"Shape: ({n}, {n}) x ({n}, {n})\")\n",
        "    print(f\"Execution Time: {exec_time:.4f} seconds\")\n",
        "    print(f\"Peak GPU Memory Usage: {peak_mem:.2f} MB\")\n",
        "    print(f\"Estimated Memory Bandwidth: {bandwidth:.2f} GB/s\")\n",
        "\n",
        "def benchmark_batched(batch_size=64, n=128):\n",
        "    print(\"\\n Batched Matrix Multiplication on GPU (A @ B)\")\n",
        "\n",
        "    A = torch.rand(batch_size, n, n, dtype=torch.float32, device='cuda')\n",
        "    B = torch.rand(batch_size, n, n, dtype=torch.float32, device='cuda')\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "    start_time = time.time()\n",
        "    C = torch.bmm(A, B)\n",
        "    torch.cuda.synchronize()\n",
        "    end_time = time.time()\n",
        "\n",
        "    exec_time = end_time - start_time\n",
        "    peak_mem = torch.cuda.max_memory_allocated() / 1e6  # MB\n",
        "    bandwidth = estimate_bandwidth(A, B, exec_time)\n",
        "\n",
        "    print(f\"Shape: ({batch_size}, {n}, {n}) x ({batch_size}, {n}, {n})\")\n",
        "    print(f\"Execution Time: {exec_time:.4f} seconds\")\n",
        "    print(f\"Peak GPU Memory Usage: {peak_mem:.2f} MB\")\n",
        "    print(f\"Estimated Memory Bandwidth: {bandwidth:.2f} GB/s\")\n",
        "\n",
        "def main():\n",
        "    if not torch.cuda.is_available():\n",
        "        print(\" CUDA not available. Please run on a GPU-enabled machine.\")\n",
        "        return\n",
        "\n",
        "    benchmark_dense(n=1024)\n",
        "    benchmark_batched(batch_size=64, n=128)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHY8X5tl0ZhK",
        "outputId": "6f0bcb63-0215-4373-bd70-847f576087cb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Dense Matrix Multiplication on GPU (A @ B)\n",
            "Shape: (1024, 1024) x (1024, 1024)\n",
            "Execution Time: 0.0127 seconds\n",
            "Peak GPU Memory Usage: 21.10 MB\n",
            "Estimated Memory Bandwidth: 0.66 GB/s\n",
            "\n",
            " Batched Matrix Multiplication on GPU (A @ B)\n",
            "Shape: (64, 128, 128) x (64, 128, 128)\n",
            "Execution Time: 0.0006 seconds\n",
            "Peak GPU Memory Usage: 21.10 MB\n",
            "Estimated Memory Bandwidth: 12.91 GB/s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####C++ (GPU accelerated)"
      ],
      "metadata": {
        "id": "lbvdvc7aKjCX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile matrix_mul_dense_and_batch.cu\n",
        "#include <iostream>\n",
        "#include <cuda_runtime.h>\n",
        "#include <cublas_v2.h>\n",
        "#include <chrono>\n",
        "\n",
        "void checkCuda(cudaError_t result, const char* msg) {\n",
        "    if (result != cudaSuccess) {\n",
        "        std::cerr << \"CUDA error in \" << msg << \": \" << cudaGetErrorString(result) << std::endl;\n",
        "        exit(EXIT_FAILURE);\n",
        "    }\n",
        "}\n",
        "\n",
        "void checkCublas(cublasStatus_t status, const char* msg) {\n",
        "    if (status != CUBLAS_STATUS_SUCCESS) {\n",
        "        std::cerr << \"cuBLAS error in \" << msg << \": \" << status << std::endl;\n",
        "        exit(EXIT_FAILURE);\n",
        "    }\n",
        "}\n",
        "\n",
        "void run_dense(int N) {\n",
        "    std::cout << \"\\n Dense Matrix Multiplication (CUDA kernel)\" << std::endl;\n",
        "\n",
        "    size_t size = N * N * sizeof(float);\n",
        "    float *A, *B, *C;\n",
        "\n",
        "    checkCuda(cudaMalloc(&A, size), \"cudaMalloc A\");\n",
        "    checkCuda(cudaMalloc(&B, size), \"cudaMalloc B\");\n",
        "    checkCuda(cudaMalloc(&C, size), \"cudaMalloc C\");\n",
        "\n",
        "    float *h_A = new float[N * N];\n",
        "    float *h_B = new float[N * N];\n",
        "    for (int i = 0; i < N * N; ++i) {\n",
        "        h_A[i] = static_cast<float>(rand()) / RAND_MAX;\n",
        "        h_B[i] = static_cast<float>(rand()) / RAND_MAX;\n",
        "    }\n",
        "\n",
        "    checkCuda(cudaMemcpy(A, h_A, size, cudaMemcpyHostToDevice), \"copy A\");\n",
        "    checkCuda(cudaMemcpy(B, h_B, size, cudaMemcpyHostToDevice), \"copy B\");\n",
        "\n",
        "    cublasHandle_t handle;\n",
        "    checkCublas(cublasCreate(&handle), \"cublasCreate\");\n",
        "\n",
        "    float alpha = 1.0f, beta = 0.0f;\n",
        "\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "    cudaEventRecord(start);\n",
        "\n",
        "    checkCublas(cublasSgemm(handle, CUBLAS_OP_N, CUBLAS_OP_N,\n",
        "                            N, N, N,\n",
        "                            &alpha,\n",
        "                            A, N,\n",
        "                            B, N,\n",
        "                            &beta,\n",
        "                            C, N),\n",
        "                \"cublasSgemm\");\n",
        "\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "\n",
        "    float ms;\n",
        "    cudaEventElapsedTime(&ms, start, stop);\n",
        "    double seconds = ms / 1000.0;\n",
        "\n",
        "    double bandwidth = (2.0 * size) / (seconds * 1e9);  // GB/s\n",
        "\n",
        "    std::cout << \"Matrix size: \" << N << \" x \" << N << std::endl;\n",
        "    std::cout << \"Execution time: \" << seconds << \" seconds\" << std::endl;\n",
        "    std::cout << \"Estimated bandwidth: \" << bandwidth << \" GB/s\" << std::endl;\n",
        "\n",
        "    cudaFree(A); cudaFree(B); cudaFree(C);\n",
        "    delete[] h_A; delete[] h_B;\n",
        "    cublasDestroy(handle);\n",
        "}\n",
        "\n",
        "void run_batched(int batch, int N) {\n",
        "    std::cout << \"\\n Batched Matrix Multiplication (cuBLAS)\" << std::endl;\n",
        "\n",
        "    size_t size = batch * N * N * sizeof(float);\n",
        "    float *A, *B, *C;\n",
        "\n",
        "    checkCuda(cudaMalloc(&A, size), \"malloc A\");\n",
        "    checkCuda(cudaMalloc(&B, size), \"malloc B\");\n",
        "    checkCuda(cudaMalloc(&C, size), \"malloc C\");\n",
        "\n",
        "    float *h_A = new float[batch * N * N];\n",
        "    float *h_B = new float[batch * N * N];\n",
        "\n",
        "    for (int i = 0; i < batch * N * N; ++i) {\n",
        "        h_A[i] = static_cast<float>(rand()) / RAND_MAX;\n",
        "        h_B[i] = static_cast<float>(rand()) / RAND_MAX;\n",
        "    }\n",
        "\n",
        "    checkCuda(cudaMemcpy(A, h_A, size, cudaMemcpyHostToDevice), \"copy A\");\n",
        "    checkCuda(cudaMemcpy(B, h_B, size, cudaMemcpyHostToDevice), \"copy B\");\n",
        "\n",
        "    cublasHandle_t handle;\n",
        "    checkCublas(cublasCreate(&handle), \"create handle\");\n",
        "\n",
        "    float alpha = 1.0f, beta = 0.0f;\n",
        "\n",
        "    long stride = N * N;\n",
        "\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "    cudaEventRecord(start);\n",
        "\n",
        "    checkCublas(cublasSgemmStridedBatched(handle,\n",
        "        CUBLAS_OP_N, CUBLAS_OP_N,\n",
        "        N, N, N,\n",
        "        &alpha,\n",
        "        A, N, stride,\n",
        "        B, N, stride,\n",
        "        &beta,\n",
        "        C, N, stride,\n",
        "        batch), \"cublasSgemmStridedBatched\");\n",
        "\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "\n",
        "    float ms;\n",
        "    cudaEventElapsedTime(&ms, start, stop);\n",
        "    double seconds = ms / 1000.0;\n",
        "\n",
        "    double bandwidth = (2.0 * size) / (seconds * 1e9);\n",
        "\n",
        "    std::cout << \"Batch size: \" << batch << \", Matrix size: \" << N << \" x \" << N << std::endl;\n",
        "    std::cout << \"Execution time: \" << seconds << \" seconds\" << std::endl;\n",
        "    std::cout << \"Estimated bandwidth: \" << bandwidth << \" GB/s\" << std::endl;\n",
        "\n",
        "    cudaFree(A); cudaFree(B); cudaFree(C);\n",
        "    delete[] h_A; delete[] h_B;\n",
        "    cublasDestroy(handle);\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    srand(42);\n",
        "    run_dense(1024);\n",
        "    run_batched(64, 128);\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rMHriTW3ONq",
        "outputId": "41c3b2c4-24b8-414b-fb89-76554a2207fe"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing matrix_mul_dense_and_batch.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -lcublas -O2 matrix_mul_dense_and_batch.cu -o matmul_gpu"
      ],
      "metadata": {
        "id": "6rOydtQb3nxz"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./matmul_gpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PltA0yoj3t01",
        "outputId": "51d84e00-1470-4da9-ed86-a94ada1c06c6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Dense Matrix Multiplication (CUDA kernel)\n",
            "Matrix size: 1024 x 1024\n",
            "Execution time: 0.0652577 seconds\n",
            "Estimated bandwidth: 0.128546 GB/s\n",
            "\n",
            " Batched Matrix Multiplication (cuBLAS)\n",
            "Batch size: 64, Matrix size: 128 x 128\n",
            "Execution time: 0.000675104 seconds\n",
            "Estimated bandwidth: 12.4257 GB/s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##CNN"
      ],
      "metadata": {
        "id": "_nWv5FmzzVYF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Numpy - GPU"
      ],
      "metadata": {
        "id": "nTMSatZQ_BGB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cupy as cp\n",
        "import time\n",
        "\n",
        "def relu(x):\n",
        "    return cp.maximum(0, x)\n",
        "\n",
        "def maxpool(x, size=2, stride=2):\n",
        "    n, c, h, w = x.shape\n",
        "    out_h = (h - size) // stride + 1\n",
        "    out_w = (w - size) // stride + 1\n",
        "    out = cp.zeros((n, c, out_h, out_w), dtype=cp.float32)\n",
        "    for i in range(out_h):\n",
        "        for j in range(out_w):\n",
        "            x_slice = x[:, :, i*stride:i*stride+size, j*stride:j*stride+size]\n",
        "            out[:, :, i, j] = cp.max(x_slice, axis=(2, 3))\n",
        "    return out\n",
        "\n",
        "def conv2d(x, w, b, stride=1, padding=0):\n",
        "    n, c_in, h, w_in = x.shape\n",
        "    c_out, _, k, _ = w.shape\n",
        "    h_out = (h + 2*padding - k) // stride + 1\n",
        "    w_out = (w_in + 2*padding - k) // stride + 1\n",
        "\n",
        "    x_padded = cp.pad(x, ((0,0), (0,0), (padding,padding), (padding,padding)), mode='constant')\n",
        "    out = cp.zeros((n, c_out, h_out, w_out), dtype=cp.float32)\n",
        "\n",
        "    for i in range(h_out):\n",
        "        for j in range(w_out):\n",
        "            x_slice = x_padded[:, :, i*stride:i*stride+k, j*stride:j*stride+k]\n",
        "            for f in range(c_out):\n",
        "                out[:, f, i, j] = cp.sum(x_slice * w[f, :, :, :], axis=(1,2,3))\n",
        "    return out + b[None, :, None, None]\n",
        "\n",
        "def flatten(x):\n",
        "    return x.reshape(x.shape[0], -1)\n",
        "\n",
        "def fully_connected(x, w, b):\n",
        "    return x @ w.T + b\n",
        "\n",
        "def estimate_bandwidth(arrays, time_seconds):\n",
        "    total_bytes = sum([a.nbytes for a in arrays])\n",
        "    return (total_bytes / 1e9) / time_seconds if time_seconds > 1e-6 else 0.0\n",
        "\n",
        "def main():\n",
        "    cp.random.seed(42)\n",
        "    batch_size = 8\n",
        "    x = cp.random.rand(batch_size, 1, 64, 64).astype(cp.float32)\n",
        "\n",
        "    w1 = cp.random.rand(16, 1, 3, 3).astype(cp.float32)\n",
        "    b1 = cp.random.rand(16).astype(cp.float32)\n",
        "\n",
        "    w2 = cp.random.rand(32, 16, 3, 3).astype(cp.float32)\n",
        "    b2 = cp.random.rand(32).astype(cp.float32)\n",
        "\n",
        "    fc1_in = 32 * 16 * 16\n",
        "    w_fc1 = cp.random.rand(128, fc1_in).astype(cp.float32)\n",
        "    b_fc1 = cp.random.rand(128).astype(cp.float32)\n",
        "\n",
        "    w_fc2 = cp.random.rand(10, 128).astype(cp.float32)\n",
        "    b_fc2 = cp.random.rand(10).astype(cp.float32)\n",
        "\n",
        "    cp.cuda.Device().synchronize()\n",
        "    cp.cuda.runtime.deviceSynchronize()\n",
        "    mem_before = cp.cuda.Device().mem_info[0]\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    out = conv2d(x, w1, b1, stride=1, padding=1)\n",
        "    out = relu(out)\n",
        "    out = maxpool(out)\n",
        "\n",
        "    out = conv2d(out, w2, b2, stride=1, padding=1)\n",
        "    out = relu(out)\n",
        "    out = maxpool(out)\n",
        "\n",
        "    out = flatten(out)\n",
        "    out = relu(fully_connected(out, w_fc1, b_fc1))\n",
        "    out = fully_connected(out, w_fc2, b_fc2)\n",
        "\n",
        "    cp.cuda.Device().synchronize()\n",
        "    exec_time = time.time() - start_time\n",
        "    mem_after = cp.cuda.Device().mem_info[0]\n",
        "    peak_mem = (mem_before - mem_after) / 1e6\n",
        "\n",
        "    bandwidth = estimate_bandwidth([x, w1, w2, out], exec_time)\n",
        "\n",
        "    print(\" Large CNN - CuPy (GPU)\")\n",
        "    print(f\"Execution Time: {exec_time:.4f} sec\")\n",
        "    print(f\"Approx. Peak GPU Memory Usage: {peak_mem:.2f} MB\")\n",
        "    print(f\"Estimated Memory Bandwidth: {bandwidth:.2f} GB/s\")\n",
        "    print(f\"Output shape: {out.shape}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mx1C9Xox-_UP",
        "outputId": "c6354f34-4130-460f-ae5e-7593f9cd338b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Large CNN - CuPy (GPU)\n",
            "Execution Time: 42.2536 sec\n",
            "Approx. Peak GPU Memory Usage: 0.00 MB\n",
            "Estimated Memory Bandwidth: 0.00 GB/s\n",
            "Output shape: (8, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Pytorch - GPU accelerated"
      ],
      "metadata": {
        "id": "QC7VkAQNASK3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "\n",
        "class CNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)   # → (B, 16, 64, 64)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)  # → (B, 32, 32, 32)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(32 * 16 * 16, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))  # → (B, 16, 32, 32)\n",
        "        x = self.pool(F.relu(self.conv2(x)))  # → (B, 32, 16, 16)\n",
        "        x = x.view(x.size(0), -1)             # → (B, 8192)\n",
        "        x = F.relu(self.fc1(x))               # → (B, 128)\n",
        "        x = self.fc2(x)                       # → (B, 10)\n",
        "        return x\n",
        "\n",
        "def estimate_bandwidth(tensors, exec_time):\n",
        "    total_bytes = sum([t.element_size() * t.numel() for t in tensors])\n",
        "    return (total_bytes / 1e9) / exec_time if exec_time > 1e-6 else 0.0\n",
        "\n",
        "def main():\n",
        "    torch.manual_seed(42)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\" Using device: {device}\")\n",
        "\n",
        "    batch_size = 8\n",
        "    model = CNNModel().to(device)\n",
        "    x = torch.randn(batch_size, 1, 64, 64, device=device)\n",
        "\n",
        "    model.eval()\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        output = model(x)\n",
        "    torch.cuda.synchronize()\n",
        "    end_time = time.time()\n",
        "\n",
        "    exec_time = end_time - start_time\n",
        "    peak_gpu_mem = torch.cuda.max_memory_allocated(device=device) / 1e6  # MB\n",
        "    bandwidth = estimate_bandwidth([x, output], exec_time)\n",
        "\n",
        "    print(\" CNN - PyTorch (GPU)\")\n",
        "    print(f\"Execution Time: {exec_time:.4f} sec\")\n",
        "    print(f\"Peak GPU Memory Usage: {peak_gpu_mem:.2f} MB\")\n",
        "    print(f\"Estimated Memory Bandwidth: {bandwidth:.2f} GB/s\")\n",
        "    print(f\"Output Shape: {output.shape}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86vqCIaHAQW9",
        "outputId": "65db2d54-2030-4a85-b7eb-9558a1fcba3e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Using device: cuda\n",
            " CNN - PyTorch (GPU)\n",
            "Execution Time: 0.5216 sec\n",
            "Peak GPU Memory Usage: 17.07 MB\n",
            "Estimated Memory Bandwidth: 0.00 GB/s\n",
            "Output Shape: torch.Size([8, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Increasing the batch size**"
      ],
      "metadata": {
        "id": "yKQiNhNIzgN3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model   --   BatchSize\t --   Winner\n",
        "\n",
        "Small   \t--     (< 32)\t   --     CPU\n",
        "\n",
        "Medium   --     (64–128)\t --    Depends\n",
        "\n",
        "Large   -- \t   ( >128) --\t        GPU"
      ],
      "metadata": {
        "id": "jCYZxeUGCgaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "\n",
        "class CNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(32 * 16 * 16, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))  # → (B, 16, 32, 32)\n",
        "        x = self.pool(F.relu(self.conv2(x)))  # → (B, 32, 16, 16)\n",
        "        x = x.view(x.size(0), -1)             # → (B, 8192)\n",
        "        x = F.relu(self.fc1(x))               # → (B, 128)\n",
        "        x = self.fc2(x)                       # → (B, 10)\n",
        "        return x\n",
        "\n",
        "def estimate_bandwidth(tensors, exec_time):\n",
        "    total_bytes = sum([t.element_size() * t.numel() for t in tensors])\n",
        "    return (total_bytes / 1e9) / exec_time if exec_time > 1e-6 else 0.0\n",
        "\n",
        "def benchmark(device, batch_size):\n",
        "    print(f\"\\n Running on: {device.upper()} with batch size {batch_size}\")\n",
        "    torch.manual_seed(42)\n",
        "    model = CNNModel().to(device)\n",
        "    x = torch.randn(batch_size, 1, 64, 64, device=device)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Warm-up\n",
        "    with torch.no_grad():\n",
        "        _ = model(x)\n",
        "\n",
        "    if device == 'cuda':\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        output = model(x)\n",
        "    if device == 'cuda':\n",
        "        torch.cuda.synchronize()\n",
        "    end_time = time.time()\n",
        "\n",
        "    exec_time = end_time - start_time\n",
        "    mem = torch.cuda.max_memory_allocated() / 1e6 if device == 'cuda' else 0\n",
        "    bandwidth = estimate_bandwidth([x, output], exec_time)\n",
        "\n",
        "    print(f\"Execution Time: {exec_time:.4f} sec\")\n",
        "    print(f\"Peak {'GPU' if device=='cuda' else 'RAM'} Memory Usage: {mem:.2f} MB\")\n",
        "    print(f\"Estimated Memory Bandwidth: {bandwidth:.2f} GB/s\")\n",
        "    print(f\"Output Shape: {output.shape}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    batch_size = 128\n",
        "    benchmark('cpu', batch_size)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        benchmark('cuda', batch_size)\n",
        "    else:\n",
        "        print(\" GPU not available. Only CPU benchmark run.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47QUCa67B_Nx",
        "outputId": "36da4fb6-c1b8-4d32-e90b-eb3e90180d49"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Running on: CPU with batch size 128\n",
            "Execution Time: 0.1356 sec\n",
            "Peak RAM Memory Usage: 0.00 MB\n",
            "Estimated Memory Bandwidth: 0.02 GB/s\n",
            "Output Shape: torch.Size([128, 10])\n",
            "\n",
            " Running on: CUDA with batch size 128\n",
            "Execution Time: 0.0029 sec\n",
            "Peak GPU Memory Usage: 81.95 MB\n",
            "Estimated Memory Bandwidth: 0.73 GB/s\n",
            "Output Shape: torch.Size([128, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Pytorch with increased batch size"
      ],
      "metadata": {
        "id": "8QgiOCaKzqZs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "\n",
        "class CNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(32 * 16 * 16, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return self.fc2(x)\n",
        "\n",
        "def estimate_bandwidth(tensors, time_sec):\n",
        "    total_bytes = sum([t.element_size() * t.numel() for t in tensors])\n",
        "    return (total_bytes / 1e9) / time_sec if time_sec > 0 else 0.0\n",
        "\n",
        "def main():\n",
        "    if not torch.cuda.is_available():\n",
        "        print(\" No CUDA device found.\")\n",
        "        return\n",
        "\n",
        "    batch_size = 128\n",
        "    device = \"cuda\"\n",
        "    model = CNNModel().to(device)\n",
        "    x = torch.randn(batch_size, 1, 64, 64, device=device)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _ = model(x)  # warm-up\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    start = time.time()\n",
        "    with torch.no_grad():\n",
        "        output = model(x)\n",
        "    torch.cuda.synchronize()\n",
        "    end = time.time()\n",
        "\n",
        "    exec_time = end - start\n",
        "    peak_gpu_mem = torch.cuda.max_memory_allocated(device=device) / 1e6\n",
        "    bandwidth = estimate_bandwidth([x, output], exec_time)\n",
        "\n",
        "    print(\" CNN - PyTorch (GPU)\")\n",
        "    print(f\"Execution Time: {exec_time:.4f} sec\")\n",
        "    print(f\"Peak GPU Memory Usage: {peak_gpu_mem:.2f} MB\")\n",
        "    print(f\"Estimated Memory Bandwidth: {bandwidth:.2f} GB/s\")\n",
        "    print(f\"Output Shape: {output.shape}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMgd9Nt5CVYy",
        "outputId": "72e336ea-ab4e-4f1c-a8cf-30d66b143726"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " CNN - PyTorch (GPU)\n",
            "Execution Time: 0.0035 sec\n",
            "Peak GPU Memory Usage: 81.95 MB\n",
            "Estimated Memory Bandwidth: 0.60 GB/s\n",
            "Output Shape: torch.Size([128, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####C++ with increased batch size"
      ],
      "metadata": {
        "id": "C1l4wBuuzuwT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile cnn_forward_cuda.cu\n",
        "// Paste the full code from the previous response here\n",
        "#include <iostream>\n",
        "#include <cuda_runtime.h>\n",
        "#include <cmath>\n",
        "\n",
        "#define IDX4(n, c, h, w, C, H, W) (((n)*(C)*(H)*(W)) + ((c)*(H)*(W)) + ((h)*(W)) + (w))\n",
        "\n",
        "__global__ void conv2d(const float* x, const float* w, const float* b, float* out,\n",
        "                       int N, int C_in, int H, int W, int C_out, int K, int H_out, int W_out, int padding) {\n",
        "    int n = blockIdx.x;\n",
        "    int f = threadIdx.x;\n",
        "\n",
        "    for (int i = 0; i < H_out; ++i) {\n",
        "        for (int j = 0; j < W_out; ++j) {\n",
        "            float sum = 0.0f;\n",
        "            for (int c = 0; c < C_in; ++c) {\n",
        "                for (int ki = 0; ki < K; ++ki) {\n",
        "                    for (int kj = 0; kj < K; ++kj) {\n",
        "                        int row = i + ki - padding;\n",
        "                        int col = j + kj - padding;\n",
        "                        if (row >= 0 && row < H && col >= 0 && col < W) {\n",
        "                            int idx_x = IDX4(n, c, row, col, C_in, H, W);\n",
        "                            int idx_w = ((f * C_in + c) * K + ki) * K + kj;\n",
        "                            sum += x[idx_x] * w[idx_w];\n",
        "                        }\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "            int idx_out = IDX4(n, f, i, j, C_out, H_out, W_out);\n",
        "            out[idx_out] = sum + b[f];\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void relu(float* x, int size) {\n",
        "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (i < size)\n",
        "        x[i] = fmaxf(0.0f, x[i]);\n",
        "}\n",
        "\n",
        "__global__ void maxpool2d(const float* x, float* out, int N, int C, int H, int W, int pool_h, int pool_w, int stride) {\n",
        "    int n = blockIdx.x;\n",
        "    int c = threadIdx.x;\n",
        "    int H_out = (H - pool_h) / stride + 1;\n",
        "    int W_out = (W - pool_w) / stride + 1;\n",
        "\n",
        "    for (int i = 0; i < H_out; ++i)\n",
        "        for (int j = 0; j < W_out; ++j) {\n",
        "            float max_val = -1e10;\n",
        "            for (int ph = 0; ph < pool_h; ++ph)\n",
        "                for (int pw = 0; pw < pool_w; ++pw) {\n",
        "                    int h = i * stride + ph;\n",
        "                    int w = j * stride + pw;\n",
        "                    int idx = IDX4(n, c, h, w, C, H, W);\n",
        "                    max_val = fmaxf(max_val, x[idx]);\n",
        "                }\n",
        "            int idx_out = IDX4(n, c, i, j, C, H_out, W_out);\n",
        "            out[idx_out] = max_val;\n",
        "        }\n",
        "}\n",
        "\n",
        "void fill_random(float* arr, int size, float scale = 0.01f) {\n",
        "    for (int i = 0; i < size; ++i)\n",
        "        arr[i] = scale * ((float)rand() / RAND_MAX);\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    const int N = 64, C = 1, H = 28, W = 28;\n",
        "    const int K1 = 3, C1 = 8;\n",
        "    const int K2 = 3, C2 = 16;\n",
        "    const int padding = 1, pool = 2, stride = 2;\n",
        "\n",
        "    int H1 = (H + 2 * padding - K1) + 1;\n",
        "    int W1 = H1;\n",
        "    int H1p = H1 / 2;\n",
        "    int W1p = W1 / 2;\n",
        "\n",
        "    int H2 = (H1p + 2 * padding - K2) + 1;\n",
        "    int W2 = H2;\n",
        "    int H2p = H2 / 2;\n",
        "    int W2p = W2 / 2;\n",
        "\n",
        "    size_t input_size = N * C * H * W * sizeof(float);\n",
        "    float *h_input = new float[N * C * H * W];\n",
        "    fill_random(h_input, N * C * H * W);\n",
        "\n",
        "    float *d_input, *d_out1, *d_out2, *d_pool1, *d_pool2;\n",
        "    cudaMalloc(&d_input, input_size);\n",
        "    cudaMemcpy(d_input, h_input, input_size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    cudaMalloc(&d_out1, N * C1 * H1 * W1 * sizeof(float));\n",
        "    cudaMalloc(&d_pool1, N * C1 * H1p * W1p * sizeof(float));\n",
        "    cudaMalloc(&d_out2, N * C2 * H2 * W2 * sizeof(float));\n",
        "    cudaMalloc(&d_pool2, N * C2 * H2p * W2p * sizeof(float));\n",
        "\n",
        "    float *w1, *b1, *w2, *b2;\n",
        "    cudaMallocManaged(&w1, C1 * C * K1 * K1 * sizeof(float));\n",
        "    cudaMallocManaged(&b1, C1 * sizeof(float));\n",
        "    cudaMallocManaged(&w2, C2 * C1 * K2 * K2 * sizeof(float));\n",
        "    cudaMallocManaged(&b2, C2 * sizeof(float));\n",
        "    fill_random(w1, C1 * C * K1 * K1);\n",
        "    fill_random(w2, C2 * C1 * K2 * K2);\n",
        "    fill_random(b1, C1);\n",
        "    fill_random(b2, C2);\n",
        "\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "    cudaEventRecord(start);\n",
        "\n",
        "    conv2d<<<N, C1>>>(d_input, w1, b1, d_out1, N, C, H, W, C1, K1, H1, W1, padding);\n",
        "    relu<<<(N*C1*H1*W1 + 255)/256, 256>>>(d_out1, N*C1*H1*W1);\n",
        "    maxpool2d<<<N, C1>>>(d_out1, d_pool1, N, C1, H1, W1, pool, pool, stride);\n",
        "\n",
        "    conv2d<<<N, C2>>>(d_pool1, w2, b2, d_out2, N, C1, H1p, W1p, C2, K2, H2, W2, padding);\n",
        "    relu<<<(N*C2*H2*W2 + 255)/256, 256>>>(d_out2, N*C2*H2*W2);\n",
        "    maxpool2d<<<N, C2>>>(d_out2, d_pool2, N, C2, H2, W2, pool, pool, stride);\n",
        "\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "    float ms = 0;\n",
        "    cudaEventElapsedTime(&ms, start, stop);\n",
        "\n",
        "    std::cout << \" CNN Forward Pass - CUDA (Batch size = \" << N << \")\\n\";\n",
        "    std::cout << \"Execution Time: \" << ms / 1000.0f << \" seconds\\n\";\n",
        "    std::cout << \"Output shape: (\" << N << \", \" << C2 << \", \" << H2p << \", \" << W2p << \")\\n\";\n",
        "\n",
        "    // Cleanup\n",
        "    cudaFree(d_input);\n",
        "    cudaFree(d_out1); cudaFree(d_out2);\n",
        "    cudaFree(d_pool1); cudaFree(d_pool2);\n",
        "    cudaFree(w1); cudaFree(b1);\n",
        "    cudaFree(w2); cudaFree(b2);\n",
        "    delete[] h_input;\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXxP04J7F6LR",
        "outputId": "66e13533-3fad-41a1-e46e-568729742786"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing cnn_forward_cuda.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -O2 cnn_forward_cuda.cu -o cnn_cuda\n"
      ],
      "metadata": {
        "id": "42h1hmHQGCRA"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./cnn_cuda"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEsF1WHIHEWg",
        "outputId": "8c0f1e4e-aa3a-41bf-f9ac-c36902a58e88"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " CNN Forward Pass - CUDA (Batch size = 64)\n",
            "Execution Time: 0.0111805 seconds\n",
            "Output shape: (64, 16, 7, 7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##GRADIENT COMPUTATION"
      ],
      "metadata": {
        "id": "8Dj1J2p4z1V7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Numpy"
      ],
      "metadata": {
        "id": "RpzEx-2kz394"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cupy as cp\n",
        "import time\n",
        "import os\n",
        "\n",
        "def quadratic_function(x):\n",
        "    return cp.sum(x ** 2)\n",
        "\n",
        "def compute_gradient_gpu(f, x, h=1e-5):\n",
        "    grad = cp.zeros_like(x)\n",
        "    for i in range(x.size):\n",
        "        x_ph = x.copy()\n",
        "        x_mh = x.copy()\n",
        "        x_ph[i] += h\n",
        "        x_mh[i] -= h\n",
        "        grad[i] = (f(x_ph) - f(x_mh)) / (2 * h)\n",
        "    return grad\n",
        "\n",
        "def estimate_bandwidth(arrays, exec_time):\n",
        "    total_bytes = sum([a.nbytes for a in arrays])\n",
        "    return (total_bytes / 1e9) / exec_time if exec_time > 1e-6 else 0.0\n",
        "\n",
        "def main():\n",
        "    dim = 100_000\n",
        "    num_iters = 3\n",
        "\n",
        "    x = cp.random.rand(dim, dtype=cp.float64)\n",
        "\n",
        "    cp.cuda.runtime.deviceSynchronize()\n",
        "    cp.cuda.Device(0).synchronize()\n",
        "    cp.cuda.runtime.deviceSynchronize()\n",
        "\n",
        "    start = time.time()\n",
        "    for _ in range(num_iters):\n",
        "        grad = compute_gradient_gpu(quadratic_function, x.copy())\n",
        "    cp.cuda.Device(0).synchronize()\n",
        "    end = time.time()\n",
        "\n",
        "    exec_time = end - start\n",
        "    bandwidth = estimate_bandwidth([x, grad], exec_time)\n",
        "    peak_gpu_mem = cp.get_default_memory_pool().used_bytes() / 1e6\n",
        "\n",
        "    print(\" GPU Gradient - CuPy\")\n",
        "    print(f\"Input Dimension: {dim}\")\n",
        "    print(f\"Iterations: {num_iters}\")\n",
        "    print(f\"Execution Time: {exec_time:.4f} sec\")\n",
        "    print(f\"Estimated Memory Bandwidth: {bandwidth:.4f} GB/s\")\n",
        "    print(f\"Approx GPU Memory Usage: {peak_gpu_mem:.2f} MB\")\n",
        "    print(f\"Gradient Norm: {cp.linalg.norm(grad):.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHsCS-cEIFXO",
        "outputId": "2045c158-9fc5-47a1-b7e4-ea9d32520a91"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " GPU Gradient - CuPy\n",
            "Input Dimension: 100000\n",
            "Iterations: 3\n",
            "Execution Time: 298.9970 sec\n",
            "Estimated Memory Bandwidth: 0.0000 GB/s\n",
            "Approx GPU Memory Usage: 1.60 MB\n",
            "Gradient Norm: 364.3787\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####PyTorch"
      ],
      "metadata": {
        "id": "TAkpL-U2z77A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "\n",
        "def quadratic_function(x):\n",
        "    return torch.sum(x ** 2)\n",
        "\n",
        "def compute_gradient_gpu(f, x, h=1e-5):\n",
        "    grad = torch.zeros_like(x)\n",
        "    for i in range(x.size(0)):\n",
        "        x_ph = x.clone()\n",
        "        x_mh = x.clone()\n",
        "        x_ph[i] += h\n",
        "        x_mh[i] -= h\n",
        "        grad[i] = (f(x_ph) - f(x_mh)) / (2 * h)\n",
        "    return grad\n",
        "\n",
        "def estimate_bandwidth(tensors, exec_time):\n",
        "    total_bytes = sum([t.element_size() * t.numel() for t in tensors])\n",
        "    return (total_bytes / 1e9) / exec_time if exec_time > 1e-6 else 0.0\n",
        "\n",
        "def main():\n",
        "    dim = 100_000\n",
        "    num_iters = 3\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    x = torch.rand(dim, dtype=torch.float64, device=device)\n",
        "\n",
        "    # Warm-up\n",
        "    torch.cuda.synchronize()\n",
        "    _ = compute_gradient_gpu(quadratic_function, x.clone())\n",
        "\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    torch.cuda.synchronize()\n",
        "    start = time.time()\n",
        "\n",
        "    for _ in range(num_iters):\n",
        "        grad = compute_gradient_gpu(quadratic_function, x.clone())\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    end = time.time()\n",
        "\n",
        "    exec_time = end - start\n",
        "    bandwidth = estimate_bandwidth([x, grad], exec_time)\n",
        "    peak_gpu_mem = torch.cuda.max_memory_allocated() / 1e6  # in MB\n",
        "\n",
        "    print(\" Gradient Computation - PyTorch (GPU)\")\n",
        "    print(f\"Input Dimension     : {dim}\")\n",
        "    print(f\"Iterations          : {num_iters}\")\n",
        "    print(f\"Execution Time      : {exec_time:.4f} sec\")\n",
        "    print(f\"Peak GPU Memory     : {peak_gpu_mem:.2f} MB\")\n",
        "    print(f\"Estimated Bandwidth : {bandwidth:.4f} GB/s\")\n",
        "    print(f\"Gradient Norm       : {torch.linalg.norm(grad).item():.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQMehHtC4fGg",
        "outputId": "c05aa8c6-becc-4d90-b6ba-7cad9ea213e4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Gradient Computation - PyTorch (GPU)\n",
            "Input Dimension     : 100000\n",
            "Iterations          : 3\n",
            "Execution Time      : 53.3157 sec\n",
            "Peak GPU Memory     : 14.12 MB\n",
            "Estimated Bandwidth : 0.0000 GB/s\n",
            "Gradient Norm       : 364.8179\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####C++"
      ],
      "metadata": {
        "id": "eFQW4pCuz-pP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile gradient_gpu.cu\n",
        "//  Paste the full CUDA code here (see above)\n",
        "#include <iostream>\n",
        "#include <cmath>\n",
        "#include <vector>\n",
        "#include <chrono>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "using namespace std;\n",
        "using namespace chrono;\n",
        "\n",
        "__global__ void compute_gradient(double* x, double* grad, int dim, double h) {\n",
        "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (i < dim) {\n",
        "        double orig = x[i];\n",
        "        double x_ph = orig + h;\n",
        "        double x_mh = orig - h;\n",
        "        grad[i] = ((x_ph * x_ph) - (x_mh * x_mh)) / (2.0 * h);  // since f(x) = sum(x_i^2)\n",
        "    }\n",
        "}\n",
        "\n",
        "double norm(const vector<double>& v) {\n",
        "    double sum_sq = 0.0;\n",
        "    for (double val : v)\n",
        "        sum_sq += val * val;\n",
        "    return sqrt(sum_sq);\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int dim = 100000;\n",
        "    int iters = 3;\n",
        "    double h = 1e-5;\n",
        "\n",
        "    vector<double> x(dim);\n",
        "    for (int i = 0; i < dim; ++i)\n",
        "        x[i] = static_cast<double>(rand()) / RAND_MAX;\n",
        "\n",
        "    double *d_x, *d_grad;\n",
        "    cudaMalloc(&d_x, dim * sizeof(double));\n",
        "    cudaMalloc(&d_grad, dim * sizeof(double));\n",
        "    cudaMemcpy(d_x, x.data(), dim * sizeof(double), cudaMemcpyHostToDevice);\n",
        "\n",
        "    int blockSize = 256;\n",
        "    int gridSize = (dim + blockSize - 1) / blockSize;\n",
        "\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "    cudaEventRecord(start);\n",
        "\n",
        "    for (int i = 0; i < iters; ++i)\n",
        "        compute_gradient<<<gridSize, blockSize>>>(d_x, d_grad, dim, h);\n",
        "\n",
        "    cudaDeviceSynchronize();\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "\n",
        "    float ms = 0.0f;\n",
        "    cudaEventElapsedTime(&ms, start, stop);\n",
        "    double seconds = ms / 1000.0;\n",
        "\n",
        "    vector<double> grad(dim);\n",
        "    cudaMemcpy(grad.data(), d_grad, dim * sizeof(double), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    double total_bytes = 2.0 * dim * sizeof(double) * iters;\n",
        "    double bandwidth = (total_bytes / 1e9) / seconds;\n",
        "\n",
        "    cout << \" Gradient Computation - CUDA (GPU)\" << endl;\n",
        "    cout << \"Input Dimension     : \" << dim << endl;\n",
        "    cout << \"Iterations          : \" << iters << endl;\n",
        "    cout << \"Execution Time      : \" << seconds << \" sec\" << endl;\n",
        "    cout << \"Estimated Bandwidth : \" << bandwidth << \" GB/s\" << endl;\n",
        "    cout << \"Gradient Norm       : \" << norm(grad) << endl;\n",
        "\n",
        "    cudaFree(d_x);\n",
        "    cudaFree(d_grad);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDpjhpGm8GHH",
        "outputId": "898f9b7f-53fc-46ee-fce8-b66891f509d2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing gradient_gpu.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -O2 gradient_gpu.cu -o gradient_gpu"
      ],
      "metadata": {
        "id": "VbjLizT38PBL"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./gradient_gpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HiorX9xF8Ttd",
        "outputId": "f59158f9-f59f-4cf1-dc95-865807418e8c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Gradient Computation - CUDA (GPU)\n",
            "Input Dimension     : 100000\n",
            "Iterations          : 3\n",
            "Execution Time      : 0.0130922 sec\n",
            "Estimated Bandwidth : 0.36663 GB/s\n",
            "Gradient Norm       : 0\n"
          ]
        }
      ]
    }
  ]
}